<!DOCTYPE html>
  
  
  
  
   <html class="no-js"> 

  <head lang="en-us">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=10" />
    <title>Extract - Arc</title>
    <meta name="generator" content="Hugo 0.41" />

    
    <meta name="description" content="Arc is an opinionated framework for defining data pipelines which are predictable, repeatable and manageable.">
    
    <link rel="canonical" href="https://aglenergy.github.io/arc/extract/">
    
    <meta name="author" content="au.com.agl.arc">
    

    <meta property="og:url" content="https://aglenergy.github.io/arc/extract/">
    <meta property="og:title" content="Arc">
    <meta property="og:image" content="https://aglenergy.github.io/arc/images/logo.png">
    <meta name="apple-mobile-web-app-title" content="Arc">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <link rel="shortcut icon" type="image/x-icon" href="https://aglenergy.github.io/arc/images/favicon.ico">
    <link rel="icon" type="image/x-icon" href="https://aglenergy.github.io/arc/images/favicon.ico">

    <style>
      @font-face {
        font-family: 'Icon';
        src: url('https://aglenergy.github.io/arc/fonts/icon.eot');
        src: url('https://aglenergy.github.io/arc/fonts/icon.eot')
               format('embedded-opentype'),
             url('https://aglenergy.github.io/arc/fonts/icon.woff')
               format('woff'),
             url('https://aglenergy.github.io/arc/fonts/icon.ttf')
               format('truetype'),
             url('https://aglenergy.github.io/arc/fonts/icon.svg')
               format('svg');
        font-weight: normal;
        font-style: normal;
      }
    </style>

    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/application.css">
    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/temporary.css">
    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/palettes.css">
    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/highlight/highlight.css">

    
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ubuntu:400,700|Ubuntu&#43;Mono">
    <style>
      body, input {
        font-family: 'Ubuntu', Helvetica, Arial, sans-serif;
      }
      pre, code {
        font-family: 'Ubuntu Mono', 'Courier New', 'Courier', monospace;
      }
    </style>

    
    <script src="https://aglenergy.github.io/arc/javascripts/modernizr.js"></script>

    

  </head>
  <body class="palette-primary-red palette-accent-teal">



	
	


<div class="backdrop">
	<div class="backdrop-paper"></div>
</div>

<input class="toggle" type="checkbox" id="toggle-drawer">
<input class="toggle" type="checkbox" id="toggle-search">
<label class="toggle-button overlay" for="toggle-drawer"></label>

<header class="header">
	<nav aria-label="Header">
  <div class="bar default">
    <div class="button button-menu" role="button" aria-label="Menu">
      <label class="toggle-button icon icon-menu" for="toggle-drawer">
        <span></span>
      </label>
    </div>
    <div class="stretch">
      <div class="title">
        Extract
      </div>
    </div>

    

    
    <div class="button button-github" role="button" aria-label="GitHub">
      <a href="https://github.com/aglenergy/arc" title="@https://github.com/aglenergy/arc on GitHub" target="_blank" class="toggle-button icon icon-github"></a>
    </div>
    
    
        
  </div>
  <div class="bar search">
    <div class="button button-close" role="button" aria-label="Close">
      <label class="toggle-button icon icon-back" for="toggle-search"></label>
    </div>
    <div class="stretch">
      <div class="field">
        <input class="query" type="text" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck>
      </div>
    </div>
    <div class="button button-reset" role="button" aria-label="Search">
      <button class="toggle-button icon icon-close" id="reset-search"></button>
    </div>
  </div>
</nav>
</header>

<main class="main">
	<div class="drawer">
		<nav aria-label="Navigation">
  <a href="https://aglenergy.github.io/arc/" class="project">
    <div class="banner">
      
      <div class="logo">
        <img src="https://aglenergy.github.io/arc/images/logo.png">
      </div>
      
      <div class="name">
        <strong>Arc 
          <span class="version">1.0.9</span></strong>
        
        <br> aglenergy/arc 
      </div>
    </div>
  </a>

  <div class="scrollable">
    <div class="wrapper">
      

      <div class="toc">
        
        <ul>
          




<li>
  
    



<a  title="Tutorial" href="https://aglenergy.github.io/arc/tutorial/">
	
	Tutorial
</a>



  
</li>



<li>
  
    



<a class="current" title="Extract" href="https://aglenergy.github.io/arc/extract/">
	
	Extract
</a>


<ul id="scrollspy">
</ul>


  
</li>



<li>
  
    



<a  title="Transform" href="https://aglenergy.github.io/arc/transform/">
	
	Transform
</a>



  
</li>



<li>
  
    



<a  title="Load" href="https://aglenergy.github.io/arc/load/">
	
	Load
</a>



  
</li>



<li>
  
    



<a  title="Execute" href="https://aglenergy.github.io/arc/execute/">
	
	Execute
</a>



  
</li>



<li>
  
    



<a  title="Validate" href="https://aglenergy.github.io/arc/validate/">
	
	Validate
</a>



  
</li>



<li>
  
    



<a  title="Metadata" href="https://aglenergy.github.io/arc/metadata/">
	
	Metadata
</a>



  
</li>



<li>
  
    



<a  title="Partials" href="https://aglenergy.github.io/arc/partials/">
	
	Partials
</a>



  
</li>



<li>
  
    



<a  title="Patterns" href="https://aglenergy.github.io/arc/patterns/">
	
	Patterns
</a>



  
</li>



<li>
  
    



<a  title="Contributing" href="https://aglenergy.github.io/arc/contributing/">
	
	Contributing
</a>



  
</li>



<li>
  
    



<a  title="License" href="https://aglenergy.github.io/arc/license/">
	
	License
</a>



  
</li>


        </ul>
         
        <hr>
        <span class="section">The author</span>

        <ul>
           
          <li>
            <a href="https://github.com/aglenergy" target="_blank" title="@aglenergy on GitHub">
              @aglenergy on GitHub
            </a>
          </li>
           
        </ul>
        
      </div>
    </div>
  </div>
</nav>
	</div>

	<article class="article">
		<div class="wrapper">
			<h1>Extract </h1>

			

<p><code>*Extract</code> stages read in data from a database or file system.</p>

<p><code>*Extract</code> stages should meet this criteria:</p>

<ul>
<li>Read data from local or remote filesystems and return a <code>DataFrame</code>.</li>
<li>Do not <a href="../transform">transform/mutate</a> the data.</li>
<li>Allow for <a href="http://www.dbms2.com/2014/07/15/the-point-of-predicate-pushdown/">Predicate Pushdown</a> depending on data source.</li>
</ul>

<p>File based <code>*Extract</code> stages can accept <code>glob</code> patterns as input filenames:</p>

<table>
<thead>
<tr>
<th>Pattern</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>*</code></td>
<td>Matches zero or more characters.</td>
</tr>

<tr>
<td><code>?</code></td>
<td>Matches any single character.</td>
</tr>

<tr>
<td><code>[abc]</code></td>
<td>Matches a single character in the set <code>{a, b, c}</code>.</td>
</tr>

<tr>
<td><code>[a-b]</code></td>
<td>Matches a single character from the character range <code>{a...b}</code>.</td>
</tr>

<tr>
<td><code>[^a-b]</code></td>
<td>Matches a single character that is not from character set or range <code>{a...b}</code>.</td>
</tr>

<tr>
<td><code>{a,b}</code></td>
<td>Matches either expression <code>a</code> or <code>b</code>.</td>
</tr>

<tr>
<td><code>\c</code></td>
<td>Removes (escapes) any special meaning of character <code>c</code>.</td>
</tr>

<tr>
<td><code>{ab,c{de, fg}}</code></td>
<td>Matches a string from the string set <code>{ab, cde, cfg}</code>.</td>
</tr>
</tbody>
</table>

<p>Spark will automatically match file extensions of <code>.zip</code>, <code>.bz2&quot;</code>, <code>.deflate</code> and <code>.gz</code> and perform decompression automatically.</p>

<h2 id="avroextract">AvroExtract</h2>

<h5 id="since-1-0-0">Since: 1.0.0</h5>

<p>The <code>AvroExtract</code> stage reads one or more <a href="https://avro.apache.org/">Apache Avro</a> files and returns a <code>DataFrame</code>.</p>

<h3 id="parameters">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the input Avro files.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td><p>Used for multiple purposes:</p>

<ul>
<li><p>Can be used to set metadata on a the extracted <code>DataFrame</code>. Note this will overwrite the existing metadata if it exists.</p></li>

<li><p>Can be used to specify a schema in case of no input files. This stage will create an empty <code>DataFrame</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</p></li>
</ul>
</td>
</tr>

<tr>
<td>schemaView</td>
<td>URI</td>
<td>false</td>
<td>Similaar to <code>schemaURI</code> but allows the schema to be passed in as another <code>DataFrame</code>. It takes precedence over <code>schemaURI</code> if provided.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The maximum number of partitions that can be used for parallelism in table reading and writing.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>contiguousIndex</td>
<td>Boolean</td>
<td>false</td>
<td>When loading a file two additional <code>metadata</code> fields are added to each record: <code>_filename</code> and <code>_index</code> (row number in the file). These fields are automatically included as they are very useful when trying to understand where certain data came from when consuming the data downstream.<br><br>The computational cost of adding the <code>_index</code> column in a distributed execution engine like Spark means that sometimes it is not worth the time/expense of precisely resolving the row number. By setting <code>contiguousIndex</code> equal to <code>false</code> Spark will include a different field <code>_monotonically_increasing_id</code> which is a non-sequential/non-contiguous identifier from which <code>_index</code> can be derived later but will not incur the same cost penalty of resolving <code>_index</code>.<br><br>Default: true.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;AvroExtract&quot;,
    &quot;name&quot;: &quot;load customer avro extract&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputURI&quot;: &quot;hdfs://input_data/customer/*.avro&quot;,
    &quot;outputView&quot;: &quot;customer&quot;,            
    &quot;persist&quot;: false,
    &quot;authentication&quot;: {
        ...
    },    
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="bytesextract">BytesExtract</h2>

<h5 id="since-1-0-9">Since: 1.0.9</h5>

<p>The <code>BytesExtract</code> stage reads one or more binary files and returns a <code>DataFrame</code> containing the file path (named <code>path</code>) and a <code>Array[Byte]</code> of the file contents (named <code>raw_content</code>).</p>

<h3 id="parameters-1">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>input</td>
<td>URI</td>
<td>true</td>
<td>URI/Glob of the input binary files.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The maximum number of partitions that can be used for parallelism in table reading and writing.</td>
</tr>

<tr>
<td>contiguousIndex</td>
<td>Boolean</td>
<td>false</td>
<td>When loading a file two additional <code>metadata</code> fields are added to each record: <code>_filename</code> and <code>_index</code> (row number in the file). These fields are automatically included as they are very useful when trying to understand where certain data came from when consuming the data downstream.<br><br>The computational cost of adding the <code>_index</code> column in a distributed execution engine like Spark means that sometimes it is not worth the time/expense of precisely resolving the row number. By setting <code>contiguousIndex</code> equal to <code>false</code> Spark will include a different field <code>_monotonically_increasing_id</code> which is a non-sequential/non-contiguous identifier from which <code>_index</code> can be derived later but will not incur the same cost penalty of resolving <code>_index</code>.<br><br>Default: true.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-1">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;BytesExtract&quot;,
    &quot;name&quot;: &quot;load images from the customer vehicle photos directory&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;input&quot;: &quot;hdfs://input_data/customer/vehicles/*.jpg&quot;,
    &quot;outputView&quot;: &quot;customer_vehicles_photos&quot;,            
    &quot;persist&quot;: false,
    &quot;numPartitions&quot;: 10,
    &quot;authentication&quot;: {
        ...
    },    
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="delimitedextract">DelimitedExtract</h2>

<h5 id="since-1-0-0-1">Since: 1.0.0</h5>

<p>The <code>DelimitedExtract</code> stage reads either one or more delimited text files or an input <code>Dataset[String]</code> and returns a <code>DataFrame</code>. <code>DelimitedExtract</code> will always set the underlying Spark configuration option of <code>inferSchema</code> to <code>false</code> to ensure consistent results.</p>

<h3 id="parameters-2">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>false*</td>
<td>Name of the incoming Spark dataset. If not present <code>inputURI</code> is requred.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>false*</td>
<td>URI/Glob of the input delimited text files. If not present <code>inputView</code> is requred.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td><p>Used for multiple purposes:</p>

<ul>
<li><p>Can be used to set metadata on a the extracted <code>DataFrame</code>. Note this will overwrite the existing metadata if it exists.</p></li>

<li><p>Can be used to specify a schema in case of no input files. This stage will create an empty <code>DataFrame</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</p></li>
</ul>
</td>
</tr>

<tr>
<td>schemaView</td>
<td>URI</td>
<td>false</td>
<td>Similaar to <code>schemaURI</code> but allows the schema to be passed in as another <code>DataFrame</code>. It takes precedence over <code>schemaURI</code> if provided.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The maximum number of partitions that can be used for parallelism in table reading and writing.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>delimiter</td>
<td>String</td>
<td>true</td>
<td>The type of delimiter in the file. Supported values: <code>Comma</code>, <code>Pipe</code>, <code>DefaultHive</code>. <code>DefaultHive</code> is  ASCII character 1, the default delimiter for Apache Hive extracts.</td>
</tr>

<tr>
<td>quote</td>
<td>String</td>
<td>true</td>
<td>The type of quoting in the file. Supported values: <code>None</code>, <code>SingleQuote</code>, <code>DoubleQuote</code>.</td>
</tr>

<tr>
<td>header</td>
<td>Boolean</td>
<td>true</td>
<td>Whether or not the dataset contains a header row. If available the output dataset will have named columns otherwise columns will be named <code>_col1</code>, <code>_col2</code> &hellip; <code>_colN</code>.</td>
</tr>

<tr>
<td>contiguousIndex</td>
<td>Boolean</td>
<td>false</td>
<td>When loading a file two additional <code>metadata</code> fields are added to each record: <code>_filename</code> and <code>_index</code> (row number in the file). These fields are automatically included as they are very useful when trying to understand where certain data came from when consuming the data downstream.<br><br>The computational cost of adding the <code>_index</code> column in a distributed execution engine like Spark means that sometimes it is not worth the time/expense of precisely resolving the row number. By setting <code>contiguousIndex</code> equal to <code>false</code> Spark will include a different field <code>_monotonically_increasing_id</code> which is a non-sequential/non-contiguous identifier from which <code>_index</code> can be derived later but will not incur the same cost penalty of resolving <code>_index</code>.<br><br>Default: true.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-2">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;DelimitedExtract&quot;,
    &quot;name&quot;: &quot;load customer csv extract&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputURI&quot;: &quot;hdfs://input_data/customer/*.csv&quot;,
    &quot;outputView&quot;: &quot;customer&quot;,            
    &quot;persist&quot;: false,
    &quot;delimiter&quot;: &quot;Comma&quot;,
    &quot;quote&quot; : &quot;DoubleQuote&quot;,
    &quot;header&quot;: true,
    &quot;authentication&quot;: {
        ...
    },
    &quot;params&quot;: {
    }
}
</code></pre>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;DelimitedExtract&quot;,
    &quot;name&quot;: &quot;split customer record extract&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputView&quot;: &quot;customer_raw&quot;,
    &quot;outputView&quot;: &quot;customer&quot;,            
    &quot;persist&quot;: false,
    &quot;delimiter&quot;: &quot;DefaultHive&quot;,
    &quot;quote&quot; : &quot;SingleQuote&quot;,
    &quot;header&quot;: false,
    &quot;authentication&quot;: {
        ...
    },    
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="httpextract">HTTPExtract</h2>

<h5 id="since-1-0-0-2">Since: 1.0.0</h5>

<p>The <code>HTTPExtract</code> executes either a <code>GET</code> or <code>POST</code> request against a remote HTTP service and returns a <code>DataFrame</code> which will have a single row and single column holding the value of the HTTP response body.</p>

<p>This stage would typically be used with a <code>JSONExtract</code> stage by specifying <code>inputView</code> instead of <code>inputURI</code> (setting <code>multiLine</code>=<code>true</code> allows processing of JSON array responses).</p>

<h3 id="parameters-3">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the HTTP server.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The maximum number of partitions that can be used for parallelism in table reading and writing.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>method</td>
<td>String</td>
<td>false</td>
<td>The request type with valid values <code>GET</code> or <code>POST</code>.<br><br>Default: <code>GET</code>.</td>
</tr>

<tr>
<td>headers</td>
<td>Map[String, String]</td>
<td>false</td>
<td><a href="https://en.wikipedia.org/wiki/List_of_HTTP_header_fields">HTTP Headers</a> to set for the HTTP request. These are not limited to the Internet Engineering Task Force standard headers.</td>
</tr>

<tr>
<td>body</td>
<td>String</td>
<td>false</td>
<td>The request body/entity that is sent with a <code>POST</code> request.</td>
</tr>

<tr>
<td>validStatusCodes</td>
<td>Array[Integer]</td>
<td>false</td>
<td>A list of valid status codes which will result in a successful stage if the list contains the HTTP server response code. If not provided the default values are <code>[200, 201, 202]</code>.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-3">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;HTTPExtract&quot;,
    &quot;name&quot;: &quot;load customer from customer api&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputURI&quot;: &quot;http://internalserver/api/customer&quot;,
    &quot;outputView&quot;: &quot;customer&quot;,            
    &quot;persist&quot;: false,
    &quot;headers&quot;: {
        &quot;Authorization&quot;: &quot;Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==&quot;,
        &quot;custom-header&quot;: &quot;payload&quot;,
    },
    &quot;validStatusCodes&quot;: [200],
    &quot;method&quot;: &quot;GET&quot;,
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="jdbcextract">JDBCExtract</h2>

<h5 id="since-1-0-0-3">Since: 1.0.0</h5>

<p>The <code>JDBCExtract</code> reads directly from a JDBC Database and returns a <code>DataFrame</code>. See <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#jdbc-to-other-databases">Spark JDBC documentation</a>.</p>

<h3 id="parameters-4">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td><p>Used for multiple purposes:</p>

<ul>
<li><p>Can be used to set metadata on a the extracted <code>DataFrame</code>. Note this will overwrite the existing metadata if it exists.</p></li>

<li><p>Can be used to specify a schema in case of no input files. This stage will create an empty <code>DataFrame</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</p></li>
</ul>
</td>
</tr>

<tr>
<td>schemaView</td>
<td>URI</td>
<td>false</td>
<td>Similaar to <code>schemaURI</code> but allows the schema to be passed in as another <code>DataFrame</code>. It takes precedence over <code>schemaURI</code> if provided.</td>
</tr>

<tr>
<td>jdbcURL</td>
<td>String</td>
<td>true</td>
<td>The JDBC URL to connect to. e.g., <code>jdbc:mysql://localhost:3306</code>.</td>
</tr>

<tr>
<td>tableName</td>
<td>String</td>
<td>true</td>
<td>The JDBC table that should be read. Note that anything that is valid in a <code>FROM</code> clause of a SQL query can be used, e.g. <code>(SELECT * FROM sourcetable WHERE key=value) sourcetable</code> or just <code>sourcetable</code>.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The maximum number of partitions that can be used for parallelism in table reading and writing. This also determines the maximum number of concurrent JDBC connections.</td>
</tr>

<tr>
<td>partitionColumn</td>
<td>String</td>
<td>false</td>
<td>The name of a numeric column from the table in question which defines how to partition the table when reading in parallel from multiple workers. If set <code>numPartitions</code> must also be set.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>predicates</td>
<td>Array[String]</td>
<td>false</td>
<td>A list expressions suitable for inclusion in <code>WHERE</code> clauses; each one defines one partition of the <code>DataFrame</code> to allow parallel reads.<br><br>e.g. <code>['id=1', 'id=2', 'id=3', 'id=4']</code> would create 4 parallel readers.</td>
</tr>

<tr>
<td>fetchsize</td>
<td>Integer</td>
<td>false</td>
<td>The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (eg. Oracle with 10 rows).</td>
</tr>

<tr>
<td>contiguousIndex</td>
<td>Boolean</td>
<td>false</td>
<td>When loading a file two additional <code>metadata</code> fields are added to each record: <code>_filename</code> and <code>_index</code> (row number in the file). These fields are automatically included as they are very useful when trying to understand where certain data came from when consuming the data downstream.<br><br>The computational cost of adding the <code>_index</code> column in a distributed execution engine like Spark means that sometimes it is not worth the time/expense of precisely resolving the row number. By setting <code>contiguousIndex</code> equal to <code>false</code> Spark will include a different field <code>_monotonically_increasing_id</code> which is a non-sequential/non-contiguous identifier from which <code>_index</code> can be derived later but will not incur the same cost penalty of resolving <code>_index</code>.<br><br>Default: true.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters.. Currently requires <code>user</code> and <code>password</code> to be set here - see example below.</td>
</tr>
</tbody>
</table>

<h3 id="examples-4">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;JDBCExtract&quot;,
    &quot;name&quot;: &quot;extract customer from jdbc&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;outputView&quot;: &quot;ative_customers&quot;,            
    &quot;persist&quot;: false,
    &quot;jdbcURL&quot;: &quot;jdbc:mysql://localhost/mydb&quot;,
    &quot;tableName&quot;: &quot;(SELECT * FROM customer WHERE active=TRUE) customer&quot;,
    &quot;numPartitions&quot;: 10,
    &quot;fetchsize&quot;: 1000,
    &quot;params&quot;: {
        &quot;user&quot;: &quot;mydbuser&quot;,
        &quot;password&quot;: &quot;mydbpassword&quot;,
    }
}
</code></pre>

<h2 id="jsonextract">JSONExtract</h2>

<h5 id="since-1-0-0-4">Since: 1.0.0</h5>

<p>The <code>JSONExtract</code> stage reads either one or more JSON files or an input <code>Dataset[String]</code> and returns a <code>DataFrame</code>.</p>

<h3 id="parameters-5">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>false*</td>
<td>Name of the incoming Spark dataset. If not present <code>inputURI</code> is requred.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>false*</td>
<td>URI/Glob of the input <code>json</code> files. If not present <code>inputView</code> is requred.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td><p>Used for multiple purposes:</p>

<ul>
<li><p>Can be used to set metadata on a the extracted <code>DataFrame</code>. Note this will overwrite the existing metadata if it exists.</p></li>

<li><p>Can be used to specify a schema in case of no input files. This stage will create an empty <code>DataFrame</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</p></li>
</ul>
<br><br>Additionally, by specifying the schema here, the underlying data source can skip the schema inference step, and thus speed up data loading.</td>
</tr>

<tr>
<td>schemaView</td>
<td>URI</td>
<td>false</td>
<td>Similaar to <code>schemaURI</code> but allows the schema to be passed in as another <code>DataFrame</code>. It takes precedence over <code>schemaURI</code> if provided.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The maximum number of partitions that can be used for parallelism in table reading and writing.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>multiLine</td>
<td>Boolean</td>
<td>false</td>
<td>Whether the input directory contains a single JSON object per file or multiple JSON records in a single file, one per line (see <a href="http://jsonlines.org/">JSONLines</a>.<br><br>Default: true.</td>
</tr>

<tr>
<td>contiguousIndex</td>
<td>Boolean</td>
<td>false</td>
<td>When loading a file two additional <code>metadata</code> fields are added to each record: <code>_filename</code> and <code>_index</code> (row number in the file). These fields are automatically included as they are very useful when trying to understand where certain data came from when consuming the data downstream.<br><br>The computational cost of adding the <code>_index</code> column in a distributed execution engine like Spark means that sometimes it is not worth the time/expense of precisely resolving the row number. By setting <code>contiguousIndex</code> equal to <code>false</code> Spark will include a different field <code>_monotonically_increasing_id</code> which is a non-sequential/non-contiguous identifier from which <code>_index</code> can be derived later but will not incur the same cost penalty of resolving <code>_index</code>.<br><br>Default: true.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-5">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;JSONExtract&quot;,
    &quot;name&quot;: &quot;load customer json extract&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputURI&quot;: &quot;hdfs://input_data/customer/*.json&quot;,
    &quot;outputView&quot;: &quot;customer&quot;,            
    &quot;persist&quot;: false,
    &quot;multiLine&quot;: true,
    &quot;authentication&quot;: {
        ...
    },
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="kafkaextract">KafkaExtract</h2>

<h5 id="since-1-0-8">Since: 1.0.8</h5>

<div class="admonition note">
<p class="admonition-title">Experimental</p>
<p><p>The <code>KafkaExtract</code> is currently in experimental state whilst the requirements become clearer.</p>

<p>This means this API is likely to change to better handle failures.</p>
</p>
</div>

<p>The <code>KafkaExtract</code> stage reads records from a <a href="https://kafka.apache.org/">Kafka</a> <code>topic</code> and returns a <code>DataFrame</code>. It requires a unique <code>groupID</code> to be set which on first run will consume from the <code>earliest</code> offset available in Kafka. Each subsequent run will use the offset as recorded against that <code>groupID</code>. This means that if a job fails before properly processing the data then data may need to be restarted from the earliest offset by creating a new <code>groupID</code>.</p>

<p>Can be used in conjuction with <a href="../execute/#kafkacommitexecute">KafkaCommitExecute</a> to allow quasi-transactional behaviour (with <code>autoCommit</code> set to <code>false</code>) - in that the offset commit can be deferred until certain dependent stages are sucessfully executed.</p>

<h3 id="parameters-6">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>topic</td>
<td>String</td>
<td>true</td>
<td>The target Kafka topic.</td>
</tr>

<tr>
<td>bootstrapServers</td>
<td>String</td>
<td>true</td>
<td>A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. e.g. <code>host1:port1,host2:port2,...</code></td>
</tr>

<tr>
<td>groupID</td>
<td>String</td>
<td>true</td>
<td>A string that uniquely identifies the group of consumer processes to which this consumer belongs. This will retain the offset of the job between executions.</td>
</tr>

<tr>
<td>maxPollRecords</td>
<td>Int</td>
<td>false</td>
<td>The maximum number of records returned in a single call to Kafka. Arc will then continue to poll until all records have been read.<br><br>Default: 10000.</td>
</tr>

<tr>
<td>timeout</td>
<td>Long</td>
<td>false</td>
<td>The time, in milliseconds, spent waiting in poll if data is not available in Kafka. Default: 10000.</td>
</tr>

<tr>
<td>autoCommit</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to update the offsets in Kafka automatically. To be used in conjuction with <a href="../execute/#kafkacommitexecute">KafkaCommitExecute</a> to allow quasi-transactional behaviour.<br><br>If <code>autoCommit</code> is set to <code>false</code> this stage will force <code>persist</code> equal to <code>true</code> so that Spark will not execute the Kafka extract process twice with a potentially different result (e.g. new messages added between extracts).<br><br>Default: false.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The maximum number of partitions that can be used for parallelism in table reading and writing.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-6">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;KafkaExtract&quot;,
    &quot;name&quot;: &quot;read customer records from kafka&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;outputView&quot;: &quot;customer&quot;,
    &quot;topic&quot;: &quot;customer&quot;, 
    &quot;bootstrapServers&quot;: &quot;kafka:29092&quot;, 
    &quot;groupID&quot;: &quot;spark-customer-extract-job&quot;,
    &quot;maxPollRecords&quot;: 10000,
    &quot;timeout&quot;: 0,
    &quot;autoCommit&quot;: false, 
    &quot;persist&quot;: true,
    &quot;params&quot;: {}
}
</code></pre>

<h2 id="orcextract">ORCExtract</h2>

<h5 id="since-1-0-0-5">Since: 1.0.0</h5>

<p>The <code>ORCExtract</code> stage reads one or more <a href="https://orc.apache.org/">Apache ORC</a> files and returns a <code>DataFrame</code>.</p>

<h3 id="parameters-7">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true</td>
<td>URI/Glob of the input ORC files.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td><p>Used for multiple purposes:</p>

<ul>
<li><p>Can be used to set metadata on a the extracted <code>DataFrame</code>. Note this will overwrite the existing metadata if it exists.</p></li>

<li><p>Can be used to specify a schema in case of no input files. This stage will create an empty <code>DataFrame</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</p></li>
</ul>
</td>
</tr>

<tr>
<td>schemaView</td>
<td>URI</td>
<td>false</td>
<td>Similaar to <code>schemaURI</code> but allows the schema to be passed in as another <code>DataFrame</code>. It takes precedence over <code>schemaURI</code> if provided.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The maximum number of partitions that can be used for parallelism in table reading and writing.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>contiguousIndex</td>
<td>Boolean</td>
<td>false</td>
<td>When loading a file two additional <code>metadata</code> fields are added to each record: <code>_filename</code> and <code>_index</code> (row number in the file). These fields are automatically included as they are very useful when trying to understand where certain data came from when consuming the data downstream.<br><br>The computational cost of adding the <code>_index</code> column in a distributed execution engine like Spark means that sometimes it is not worth the time/expense of precisely resolving the row number. By setting <code>contiguousIndex</code> equal to <code>false</code> Spark will include a different field <code>_monotonically_increasing_id</code> which is a non-sequential/non-contiguous identifier from which <code>_index</code> can be derived later but will not incur the same cost penalty of resolving <code>_index</code>.<br><br>Default: true.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-7">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;ParquetExtract&quot;,
    &quot;name&quot;: &quot;load customer orc extract&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputURI&quot;: &quot;hdfs://input_data/customer/*.orc&quot;,
    &quot;outputView&quot;: &quot;customer&quot;,            
    &quot;persist&quot;: false,
    &quot;authentication&quot;: {
        ...
    },    
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="parquetextract">ParquetExtract</h2>

<h5 id="since-1-0-0-6">Since: 1.0.0</h5>

<p>The <code>ParquetExtract</code> stage reads one or more <a href="https://parquet.apache.org/">Apache Parquet</a> files and returns a <code>DataFrame</code>.</p>

<h3 id="parameters-8">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true</td>
<td>URI/Glob of the input Parquet files.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td><p>Used for multiple purposes:</p>

<ul>
<li><p>Can be used to set metadata on a the extracted <code>DataFrame</code>. Note this will overwrite the existing metadata if it exists.</p></li>

<li><p>Can be used to specify a schema in case of no input files. This stage will create an empty <code>DataFrame</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</p></li>
</ul>
</td>
</tr>

<tr>
<td>schemaView</td>
<td>URI</td>
<td>false</td>
<td>Similaar to <code>schemaURI</code> but allows the schema to be passed in as another <code>DataFrame</code>. It takes precedence over <code>schemaURI</code> if provided.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The maximum number of partitions that can be used for parallelism in table reading and writing.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>contiguousIndex</td>
<td>Boolean</td>
<td>false</td>
<td>When loading a file two additional <code>metadata</code> fields are added to each record: <code>_filename</code> and <code>_index</code> (row number in the file). These fields are automatically included as they are very useful when trying to understand where certain data came from when consuming the data downstream.<br><br>The computational cost of adding the <code>_index</code> column in a distributed execution engine like Spark means that sometimes it is not worth the time/expense of precisely resolving the row number. By setting <code>contiguousIndex</code> equal to <code>false</code> Spark will include a different field <code>_monotonically_increasing_id</code> which is a non-sequential/non-contiguous identifier from which <code>_index</code> can be derived later but will not incur the same cost penalty of resolving <code>_index</code>.<br><br>Default: true.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-8">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;ParquetExtract&quot;,
    &quot;name&quot;: &quot;load customer parquet extract&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputURI&quot;: &quot;hdfs://input_data/customer/*.parquet&quot;,
    &quot;outputView&quot;: &quot;customer&quot;,            
    &quot;persist&quot;: false,
    &quot;authentication&quot;: {
        ...
    },    
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="xmlextract">XMLExtract</h2>

<h5 id="since-1-0-0-7">Since: 1.0.0</h5>

<p>The <code>XMLExtract</code> stage reads one or more XML files or an input <code>Dataset[String]</code> and returns a <code>DataFrame</code>.</p>

<h3 id="parameters-9">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>false*</td>
<td>Name of the incoming Spark dataset. If not present <code>inputURI</code> is requred.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>false*</td>
<td>URI/Glob of the input delimited  XML files. If not present <code>inputView</code> is requred.</td>
</tr>

<tr>
<td>schemaView</td>
<td>URI</td>
<td>false</td>
<td>Similaar to <code>schemaURI</code> but allows the schema to be passed in as another <code>DataFrame</code>. It takes precedence over <code>schemaURI</code> if provided.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td><p>Used for multiple purposes:</p>

<ul>
<li><p>Can be used to set metadata on a the extracted <code>DataFrame</code>. Note this will overwrite the existing metadata if it exists.</p></li>

<li><p>Can be used to specify a schema in case of no input files. This stage will create an empty <code>DataFrame</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</p></li>
</ul>
<br><br>Additionally, by specifying the schema here, the underlying data source can skip the schema inference step, and thus speed up data loading.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The maximum number of partitions that can be used for parallelism in table reading and writing.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>contiguousIndex</td>
<td>Boolean</td>
<td>false</td>
<td>When loading a file two additional <code>metadata</code> fields are added to each record: <code>_filename</code> and <code>_index</code> (row number in the file). These fields are automatically included as they are very useful when trying to understand where certain data came from when consuming the data downstream.<br><br>The computational cost of adding the <code>_index</code> column in a distributed execution engine like Spark means that sometimes it is not worth the time/expense of precisely resolving the row number. By setting <code>contiguousIndex</code> equal to <code>false</code> Spark will include a different field <code>_monotonically_increasing_id</code> which is a non-sequential/non-contiguous identifier from which <code>_index</code> can be derived later but will not incur the same cost penalty of resolving <code>_index</code>.<br><br>Default: true.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-9">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;XMLExtract&quot;,
    &quot;name&quot;: &quot;load customer xml extract&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputURI&quot;: &quot;hdfs://input_data/customer/*.xml&quot;,
    &quot;outputView&quot;: &quot;customer&quot;,            
    &quot;persist&quot;: false,
    &quot;authentication&quot;: {
        ...
    },    
    &quot;params&quot;: {
    }
}
</code></pre>


			<aside class="copyright" role="note">
				
				&copy; 2018 Released under the MIT license
				
			</aside>

			<footer class="footer">
				

<nav class="pagination" aria-label="Footer">
  <div class="previous">
  
      <a href="https://aglenergy.github.io/arc/tutorial/" title="Tutorial">
        <span class="direction">
          Previous
        </span>
        <div class="page">
          <div class="button button-previous" role="button" aria-label="Previous">
            <i class="icon icon-back"></i>
          </div>
          <div class="stretch">
            <div class="title">
              Tutorial
            </div>
          </div>
        </div>
      </a>
  
  </div>

  <div class="next">
  
      <a href="https://aglenergy.github.io/arc/transform/" title="Transform">
        <span class="direction">
          Next
        </span>
        <div class="page">
          <div class="stretch">
            <div class="title">
              Transform
            </div>
          </div>
          <div class="button button-next" role="button" aria-label="Next">
            <i class="icon icon-forward"></i>
          </div>
        </div>
      </a>
  
  </div>
</nav>





			</footer>
		</div>
	</article>

	<div class="results" role="status" aria-live="polite">
		<div class="scrollable">
			<div class="wrapper">
				<div class="meta"></div>
				<div class="list"></div>
			</div>
		</div>
	</div>
</main>

    <script>
    
      var base_url = 'https:\/\/aglenergy.github.io\/arc\/';
      var repo_id  = 'aglenergy\/arc';
    
    </script>

    <script src="https://aglenergy.github.io/arc/javascripts/application.js"></script>
    

    <script>
      /* Add headers to scrollspy */
      var headers   = document.getElementsByTagName("h2");
      var scrollspy = document.getElementById('scrollspy');

      if(scrollspy) {
        if(headers.length > 0) {
          for(var i = 0; i < headers.length; i++) {
            var li = document.createElement("li");
            li.setAttribute("class", "anchor");

            var a  = document.createElement("a");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", headers[i].innerHTML);
            a.innerHTML = headers[i].innerHTML;

            li.appendChild(a)
            scrollspy.appendChild(li);
          }
        } else {
          scrollspy.parentElement.removeChild(scrollspy)
        }


        /* Add permanent link next to the headers */
        var headers = document.querySelectorAll("h1, h2, h3, h4, h5, h6");

        for(var i = 0; i < headers.length; i++) {
            var a = document.createElement("a");
            a.setAttribute("class", "headerlink");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", "Permanent link")
            a.innerHTML = "#";
            headers[i].appendChild(a);
        }
      }
    </script>

    

    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </body>
</html>

