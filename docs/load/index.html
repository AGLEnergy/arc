<!DOCTYPE html>
  
  
  
  
   <html class="no-js"> 

  <head lang="en-us">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=10" />
    <title>Load - Arc</title>
    <meta name="generator" content="Hugo 0.41" />

    
    <meta name="description" content="Arc is an opinionated framework for defining data pipelines which are predictable, repeatable and manageable.">
    
    <link rel="canonical" href="https://aglenergy.github.io/arc/load/">
    
    <meta name="author" content="au.com.agl.arc">
    

    <meta property="og:url" content="https://aglenergy.github.io/arc/load/">
    <meta property="og:title" content="Arc">
    <meta property="og:image" content="https://aglenergy.github.io/arc/images/logo.png">
    <meta name="apple-mobile-web-app-title" content="Arc">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <link rel="shortcut icon" type="image/x-icon" href="https://aglenergy.github.io/arc/images/favicon.ico">
    <link rel="icon" type="image/x-icon" href="https://aglenergy.github.io/arc/images/favicon.ico">

    <style>
      @font-face {
        font-family: 'Icon';
        src: url('https://aglenergy.github.io/arc/fonts/icon.eot');
        src: url('https://aglenergy.github.io/arc/fonts/icon.eot')
               format('embedded-opentype'),
             url('https://aglenergy.github.io/arc/fonts/icon.woff')
               format('woff'),
             url('https://aglenergy.github.io/arc/fonts/icon.ttf')
               format('truetype'),
             url('https://aglenergy.github.io/arc/fonts/icon.svg')
               format('svg');
        font-weight: normal;
        font-style: normal;
      }
    </style>

    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/application.css">
    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/temporary.css">
    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/palettes.css">
    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/highlight/highlight.css">

    
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ubuntu:400,700|Ubuntu&#43;Mono">
    <style>
      body, input {
        font-family: 'Ubuntu', Helvetica, Arial, sans-serif;
      }
      pre, code {
        font-family: 'Ubuntu Mono', 'Courier New', 'Courier', monospace;
      }
    </style>

    
    <script src="https://aglenergy.github.io/arc/javascripts/modernizr.js"></script>

    

  </head>
  <body class="palette-primary-red palette-accent-teal">



	
	


<div class="backdrop">
	<div class="backdrop-paper"></div>
</div>

<input class="toggle" type="checkbox" id="toggle-drawer">
<input class="toggle" type="checkbox" id="toggle-search">
<label class="toggle-button overlay" for="toggle-drawer"></label>

<header class="header">
	<nav aria-label="Header">
  <div class="bar default">
    <div class="button button-menu" role="button" aria-label="Menu">
      <label class="toggle-button icon icon-menu" for="toggle-drawer">
        <span></span>
      </label>
    </div>
    <div class="stretch">
      <div class="title">
        Load
      </div>
    </div>

    

    
    <div class="button button-github" role="button" aria-label="GitHub">
      <a href="https://github.com/aglenergy/arc" title="@https://github.com/aglenergy/arc on GitHub" target="_blank" class="toggle-button icon icon-github"></a>
    </div>
    
    
        
  </div>
  <div class="bar search">
    <div class="button button-close" role="button" aria-label="Close">
      <label class="toggle-button icon icon-back" for="toggle-search"></label>
    </div>
    <div class="stretch">
      <div class="field">
        <input class="query" type="text" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck>
      </div>
    </div>
    <div class="button button-reset" role="button" aria-label="Search">
      <button class="toggle-button icon icon-close" id="reset-search"></button>
    </div>
  </div>
</nav>
</header>

<main class="main">
	<div class="drawer">
		<nav aria-label="Navigation">
  <a href="https://aglenergy.github.io/arc/" class="project">
    <div class="banner">
      
      <div class="logo">
        <img src="https://aglenergy.github.io/arc/images/logo.png">
      </div>
      
      <div class="name">
        <strong>Arc 
          <span class="version">1.3.0</span></strong>
        
        <br> aglenergy/arc 
      </div>
    </div>
  </a>

  <div class="scrollable">
    <div class="wrapper">
      

      <div class="toc">
        
        <ul>
          




<li>
  
    



<a  title="Tutorial" href="https://aglenergy.github.io/arc/tutorial/">
	
	Tutorial
</a>



  
</li>



<li>
  
    



<a  title="Extract" href="https://aglenergy.github.io/arc/extract/">
	
	Extract
</a>



  
</li>



<li>
  
    



<a  title="Transform" href="https://aglenergy.github.io/arc/transform/">
	
	Transform
</a>



  
</li>



<li>
  
    



<a class="current" title="Load" href="https://aglenergy.github.io/arc/load/">
	
	Load
</a>


<ul id="scrollspy">
</ul>


  
</li>



<li>
  
    



<a  title="Execute" href="https://aglenergy.github.io/arc/execute/">
	
	Execute
</a>



  
</li>



<li>
  
    



<a  title="Validate" href="https://aglenergy.github.io/arc/validate/">
	
	Validate
</a>



  
</li>



<li>
  
    



<a  title="Metadata" href="https://aglenergy.github.io/arc/metadata/">
	
	Metadata
</a>



  
</li>



<li>
  
    



<a  title="Partials" href="https://aglenergy.github.io/arc/partials/">
	
	Partials
</a>



  
</li>



<li>
  
    



<a  title="Patterns" href="https://aglenergy.github.io/arc/patterns/">
	
	Patterns
</a>



  
</li>



<li>
  
    



<a  title="Deploy" href="https://aglenergy.github.io/arc/deploy/">
	
	Deploy
</a>



  
</li>



<li>
  
    



<a  title="Extend" href="https://aglenergy.github.io/arc/extend/">
	
	Extend
</a>



  
</li>



<li>
  
    



<a  title="Contributing" href="https://aglenergy.github.io/arc/contributing/">
	
	Contributing
</a>



  
</li>



<li>
  
    



<a  title="License" href="https://aglenergy.github.io/arc/license/">
	
	License
</a>



  
</li>


        </ul>
         
        <hr>
        <span class="section">The author</span>

        <ul>
           
          <li>
            <a href="https://github.com/aglenergy" target="_blank" title="@aglenergy on GitHub">
              @aglenergy on GitHub
            </a>
          </li>
           
        </ul>
        
      </div>
    </div>
  </div>
</nav>
	</div>

	<article class="article">
		<div class="wrapper">
			<h1>Load </h1>

			

<p><code>*Load</code> stages write out Spark <code>datasets</code> to a database or file system.</p>

<p><code>*Load</code> stages should meet this criteria:</p>

<ul>
<li>Take in a single <code>dataset</code>.</li>
<li>Perform target specific validation that the dataset has been written correctly.</li>
</ul>

<h2 id="avroload">AvroLoad</h2>

<h5 id="since-1-0-0-supports-streaming-false">Since: 1.0.0 - Supports Streaming: False</h5>

<p>The <code>AvroLoad</code> writes an input <code>DataFrame</code> to a target <a href="https://avro.apache.org/">Apache Avro</a> file.</p>

<h3 id="parameters">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the Parquet file to write to.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The maximum number of partitions that can be used for parallelism in table reading and writing.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;AvroLoad&quot;,
    &quot;name&quot;: &quot;write customer records to avro&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputView&quot;: &quot;customer&quot;,
    &quot;outputURI&quot;: &quot;hdfs://datalake/raw/customer.avro&quot;,
    &quot;numPartitions&quot;: 100,
    &quot;partitionBy&quot;: [
        &quot;customer_segment&quot;,
        &quot;customer_type&quot;
    ],
    &quot;authentication&quot;: {
        ...
    },    
    &quot;saveMode&quot;: &quot;Append&quot;,
    &quot;params&quot;: {}
}
</code></pre>

<h2 id="azureeventhubsload">AzureEventHubsLoad</h2>

<h5 id="since-1-0-0-supports-streaming-false-1">Since: 1.0.0 - Supports Streaming: False</h5>

<p>The <code>AzureEventHubsLoad</code> writes an input <code>DataFrame</code> to a target <a href="https://azure.microsoft.com/en-gb/services/event-hubs/">Azure Event Hubs</a> stream. The input to this stage needs to be a single column dataset of signature <code>value: string</code> and is intended to be used after a <a href="https://aglenergy.github.io/arc/load/#jsontransform">JSONTransform</a> stage which would prepare the data for sending to the external server.</p>

<p>In the future additional Transform stages (like <code>ProtoBufTransform</code>) could be added to prepare binary payloads instead of just <code>json</code> <code>string</code>.</p>

<h3 id="parameters-1">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>namespaceName</td>
<td>String</td>
<td>true</td>
<td>The Event Hubs namespace.</td>
</tr>

<tr>
<td>eventHubName</td>
<td>String</td>
<td>true</td>
<td>The Event Hubs entity.</td>
</tr>

<tr>
<td>sharedAccessSignatureKeyName</td>
<td>String</td>
<td>true</td>
<td>The Event Hubs Shared Access Signature Key Name.</td>
</tr>

<tr>
<td>sharedAccessSignatureKey</td>
<td>String</td>
<td>true</td>
<td>The Event Hubs Shared Access Signature Key.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The maximum number of partitions that can be used for parallelism in table reading and writing. Azure EventHubs will throw a <code>ServerBusyException</code> if too many executors write to a target in parallel which can be decreased by reducing the number of partitions.</td>
</tr>

<tr>
<td>retryMinBackoff</td>
<td>Long</td>
<td>false</td>
<td>The minimum time (in seconds) for the exponential backoff algorithm to wait between retries.<br><br>Default: 0.</td>
</tr>

<tr>
<td>retryMaxBackoff</td>
<td>Long</td>
<td>false</td>
<td>The maximum time (in seconds) for the exponential backoff algorithm to wait between retries.<br><br>Default: 30.</td>
</tr>

<tr>
<td>retryCount</td>
<td>Integer</td>
<td>false</td>
<td>The maximum number of retries for the exponential backoff algorithm.<br><br>Default: 10.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-1">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;AzureEventHubsLoad&quot;,
    &quot;name&quot;: &quot;write customer records to eventhub&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputView&quot;: &quot;customer&quot;,
    &quot;namespaceName&quot;: &quot;mynamespace&quot;, 
    &quot;eventHubName&quot;: &quot;myeventhub&quot;, 
    &quot;sharedAccessSignatureKeyName&quot;: &quot;mysignaturename&quot;, 
    &quot;sharedAccessSignatureKey&quot;: &quot;ctzMq410TV3wS7upTBcunJTDLEJwMAZuFPfr0mrrA08=&quot;,
    &quot;numPartitions&quot;: 4,
    &quot;retryMinBackoff&quot;: 5,
    &quot;retryMinBackoff&quot;: 60,
    &quot;retryCount&quot;: 30,
    &quot;params&quot;: {}
}
</code></pre>

<h2 id="consoleload">ConsoleLoad</h2>

<h5 id="since-1-2-0-supports-streaming-true">Since: 1.2.0 - Supports Streaming: True</h5>

<p>The <code>ConsoleLoad</code> prints an input streaming <code>DataFrame</code> the console.</p>

<p>This stage has been included for testing Structured Streaming jobs as it can be very difficult to debug. Generally this stage would only be included when Arc is run in a test mode (i.e. the <code>environment</code> is set to <code>test</code>).</p>

<h3 id="parameters-2">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputMode</td>
<td>String</td>
<td>false</td>
<td>The output mode of the console writer. Allowed values <code>Append</code>, <code>Complete</code>, <code>Update</code>. See <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes">Output Modes</a> for full details.<br><br>Default: Append</td>
</tr>
</tbody>
</table>

<h3 id="examples-2">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;ConsoleLoad&quot;,
    &quot;name&quot;: &quot;write a streaming dataset to console&quot;,
    &quot;environments&quot;: [&quot;test&quot;],
    &quot;inputView&quot;: &quot;customer&quot;,
    &quot;outputMode&quot;: &quot;Append&quot;,
    &quot;params&quot;: {}
}
</code></pre>

<h2 id="delimitedload">DelimitedLoad</h2>

<h5 id="since-1-0-0-supports-streaming-true">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>DelimitedLoad</code> writes an input <code>DataFrame</code> to a target delimited file.</p>

<h3 id="parameters-3">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the Delimited file to write to.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The maximum number of partitions that can be used for parallelism in table reading and writing.</td>
</tr>

<tr>
<td>delimiter</td>
<td>String</td>
<td>true</td>
<td>The type of delimiter in the file. Supported values: <code>Comma</code>, <code>Pipe</code>, <code>DefaultHive</code>. <code>DefaultHive</code> is  ASCII character 1, the default delimiter for Apache Hive extracts.</td>
</tr>

<tr>
<td>quote</td>
<td>String</td>
<td>true</td>
<td>The type of quoting in the file. Supported values: <code>None</code>, <code>SingleQuote</code>, <code>DoubleQuote</code>.</td>
</tr>

<tr>
<td>header</td>
<td>Boolean</td>
<td>true</td>
<td>Whether or not the dataset contains a header row. If available the output dataset will have named columns otherwise columns will be named <code>_col1</code>, <code>_col2</code> &hellip; <code>_colN</code>.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-3">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;DelimitedLoad&quot;,
    &quot;name&quot;: &quot;write out customer csvs&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputView&quot;: &quot;customer&quot;,            
    &quot;outputURI&quot;: &quot;hdfs://input_data/customer/customer.csv&quot;,
    &quot;delimiter&quot;: &quot;Comma&quot;,
    &quot;quote&quot; : &quot;DoubleQuote&quot;,
    &quot;header&quot;: true,
    &quot;partitionBy&quot;: [&quot;active&quot;],
    &quot;numPartitions&quot;: 10,
    &quot;authentication&quot;: {
        ...
    },
    &quot;saveMode&quot;: &quot;Append&quot;,
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="httpload">HTTPLoad</h2>

<h5 id="since-1-0-0-supports-streaming-false-2">Since: 1.0.0 - Supports Streaming: False</h5>

<p>The <code>HTTPLoad</code> takes an input <code>DataFrame</code> and executes a series of <code>POST</code> requests against a remote HTTP service. The input to this stage needs to be a single column dataset of signature <code>value: string</code> and is intended to be used after a <a href="https://aglenergy.github.io/arc/load/#jsontransform">JSONTransform</a> stage which would prepare the data for sending to the external server.</p>

<p>In the future additional Transform stages (like <code>ProtoBufTransform</code>) could be added to prepare binary payloads instead of just <code>json</code> <code>string</code>.</p>

<h3 id="parameters-4">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the HTTP server.</td>
</tr>

<tr>
<td>headers</td>
<td>Map[String, String]</td>
<td>false</td>
<td><a href="https://en.wikipedia.org/wiki/List_of_HTTP_header_fields">HTTP Headers</a> to set for the HTTP request. These are not limited to the Internet Engineering Task Force standard headers.</td>
</tr>

<tr>
<td>validStatusCodes</td>
<td>Array[Integer]</td>
<td>false</td>
<td>A list of valid status codes which will result in a successful stage if the list contains the HTTP server response code. If not provided the default values are <code>[200, 201, 202]</code>. Note: all request response codes must be contained in this list for the stage to be successful.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-4">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;HTTPLoad&quot;,
    &quot;name&quot;: &quot;load customers to the customer api&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputView&quot;: &quot;customer&quot;,            
    &quot;outputURI&quot;: &quot;http://internalserver/api/customer&quot;,
    &quot;headers&quot;: {
        &quot;Authorization&quot;: &quot;Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==&quot;,
        &quot;custom-header&quot;: &quot;payload&quot;,
    },
    &quot;validStatusCodes&quot;: [200],
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="jdbcload">JDBCLoad</h2>

<h5 id="since-1-0-0-supports-streaming-true-1">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>JDBCLoad</code> writes an input <code>DataFrame</code> to a target JDBC Database. See <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#jdbc-to-other-databases">Spark JDBC documentation</a>.</p>

<p>Whilst it is possible to use <code>JDBCLoad</code> to create tables directly in the target database Spark only has a limited knowledge of the schema required in the destination database and so will translate things like <code>StringType</code> internally to a <code>TEXT</code> type in the target database (because internally Spark does not have limited length strings). The recommendation is to use a preceding <a href="../execute/#jdbcexecute">JDBCExecute</a> to execute a <code>CREATE TABLE</code> statement which creates the intended schema then inserting into that table with <code>saveMode</code> set to <code>Append</code>.</p>

<h3 id="parameters-5">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>jdbcURL</td>
<td>String</td>
<td>true</td>
<td>The JDBC URL to connect to. e.g., <code>jdbc:mysql://localhost:3306</code>.</td>
</tr>

<tr>
<td>tableName</td>
<td>String</td>
<td>true</td>
<td>The target JDBC table. Must be in <code>database</code>.<code>schema</code>.<code>table</code> format.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The maximum number of partitions that can be used for parallelism in table reading and writing. This also determines the maximum number of concurrent JDBC connections.</td>
</tr>

<tr>
<td>isolationLevel</td>
<td>String</td>
<td>false</td>
<td>The transaction isolation level, which applies to current connection. It can be one of NONE, READ_COMMITTED, READ_UNCOMMITTED, REPEATABLE_READ, or SERIALIZABLE, corresponding to standard transaction isolation levels defined by JDBC&rsquo;s Connection object, with default of READ_UNCOMMITTED. Please refer the documentation in <a href="https://docs.oracle.com/javase/8/docs/api/java/sql/Connection.html">java.sql.Connection</a>.</td>
</tr>

<tr>
<td>batchsize</td>
<td>Integer</td>
<td>false</td>
<td>The JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers.<br><br>Defaults:<br>Non-Bulkload: <code>1000</code><br>Bulkload: <code>10000</code>.</td>
</tr>

<tr>
<td>createTableOptions</td>
<td>String</td>
<td>false</td>
<td>This is a JDBC writer related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g., <code>CREATE TABLE t (name string) ENGINE=InnoDB</code>).</td>
</tr>

<tr>
<td>createTableColumnTypes</td>
<td>String</td>
<td>false</td>
<td>The database column data types to use instead of the defaults, when creating the table. Data type information should be specified in the same format as <code>CREATE TABLE</code> columns syntax (e.g: &ldquo;<code>name CHAR(64), comments VARCHAR(1024)</code>&rdquo;). The specified types should be valid spark sql data types.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>

<tr>
<td>truncate</td>
<td>Boolean</td>
<td>false</td>
<td>If using <code>SaveMode</code> equal to <code>Overwrite</code>, this additional option causes Spark to <code>TRUNCATE TABLE</code> of existing data instead of executing a <code>DELETE FROM</code> statement.</td>
</tr>

<tr>
<td>bulkload</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to enable a <code>bulk</code> copy. This is currently only available for <code>sqlserver</code> targets but more targets can be added as drivers become available.</td>
</tr>

<tr>
<td>tablock</td>
<td>Boolean</td>
<td>false</td>
<td>When in <code>bulkload</code> mode whether to set <code>TABLOCK</code> on the driver.<br><br>Default: <code>true</code>.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters.. Currently requires <code>user</code> and <code>password</code> to be set here - see example below.</td>
</tr>
</tbody>
</table>

<h3 id="examples-5">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;JDBCLoad&quot;,
    &quot;name&quot;: &quot;load active customers to web server database&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputView&quot;: &quot;ative_customers&quot;,            
    &quot;jdbcURL&quot;: &quot;jdbc:mysql://localhost/mydb&quot;,
    &quot;tableName&quot;: &quot;mydatabase.myschema.customers&quot;,
    &quot;numPartitions&quot;: 10,
    &quot;isolationLevel&quot;: &quot;READ_COMMITTED&quot;,
    &quot;batchsize&quot;: 10000,
    &quot;truncate&quot;: false,
    &quot;saveMode&quot;: &quot;Append&quot;,
    &quot;bulkload&quot;: false,
    &quot;params&quot;: {
        &quot;user&quot;: &quot;mydbuser&quot;,
        &quot;password&quot;: &quot;mydbpassword&quot;,
    }
}
</code></pre>

<h2 id="jsonload">JSONLoad</h2>

<h5 id="since-1-0-0-supports-streaming-true-2">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>JSONLoad</code> writes an input <code>DataFrame</code> to a target JSON file.</p>

<h3 id="parameters-6">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the Delimited file to write to.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-6">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;JSONLoad&quot;,
    &quot;name&quot;: &quot;load customer json extract&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;outputView&quot;: &quot;customer&quot;,            
    &quot;outputURI&quot;: &quot;hdfs://input_data/customer/customer.json&quot;,
    &quot;authentication&quot;: {
        ...
    },
    &quot;saveMode&quot;: &quot;Append&quot;,
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="kafkaload">KafkaLoad</h2>

<h5 id="since-1-0-8-supports-streaming-true">Since: 1.0.8 - Supports Streaming: True</h5>

<p>The <code>KafkaLoad</code> writes an input <code>DataFrame</code> to a target <a href="https://kafka.apache.org/">Kafka</a> <code>topic</code>. The input to this stage needs to be a single column dataset of signature <code>value: string</code> - intended to be used after a <a href="https://aglenergy.github.io/arc/load/#jsontransform">JSONTransform</a> stage - or a two columns of signature <code>key: string, value: string</code> which could be created by a <a href="https://aglenergy.github.io/arc/load/#sqltransform">SQLTransform</a> stage.</p>

<p>In the future additional Transform stages (like <code>ProtoBufTransform</code>) may be added to prepare binary payloads instead of just <code>json</code> <code>string</code>.</p>

<h3 id="parameters-7">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>topic</td>
<td>String</td>
<td>true</td>
<td>The target Kafka topic.</td>
</tr>

<tr>
<td>bootstrapServers</td>
<td>String</td>
<td>true</td>
<td>A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. e.g. <code>host1:port1,host2:port2,...</code></td>
</tr>

<tr>
<td>acks</td>
<td>Integer</td>
<td>true</td>
<td>The number of acknowledgments the producer requires the leader to have received before considering a request complete.<br><br>Alowed values:<br><code>1</code>: the leader will write the record to its local log but will respond without awaiting full acknowledgement from all followers.<br><code>0</code>:  the job will not wait for any acknowledgment from the server at all.<br><code>-1</code>:  the leader will wait for the full set of in-sync replicas to acknowledge the record (safest).</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The maximum number of partitions that can be used for parallelism in table reading and writing.</td>
</tr>

<tr>
<td>batchSize</td>
<td>Integer</td>
<td>false</td>
<td>Number of records to send in single requet to reduce number of requests to Kafka.<br><br>Default: 16384.</td>
</tr>

<tr>
<td>retries</td>
<td>Integer</td>
<td>false</td>
<td>How many times to try to resend any record whose send fails with a potentially transient error.<br><br>Default: 0.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-7">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;KafkaLoad&quot;,
    &quot;name&quot;: &quot;write customer records to kafka&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputView&quot;: &quot;customer&quot;,
    &quot;topic&quot;: &quot;customer&quot;, 
    &quot;bootstrapServers&quot;: &quot;kafka:29092&quot;, 
    &quot;acks&quot;: -1,
    &quot;numPartitions&quot;: 4,
    &quot;batchSize&quot;: 16384,
    &quot;retries&quot;: 3,
    &quot;params&quot;: {}
}
</code></pre>

<h2 id="orcload">ORCLoad</h2>

<h5 id="since-1-0-0-supports-streaming-true-3">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>ORCLoad</code> writes an input <code>DataFrame</code> to a target <a href="https://orc.apache.org/">Apache ORC</a> file.</p>

<h3 id="parameters-8">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the ORC file to write to.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The maximum number of partitions that can be used for parallelism in table reading and writing.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-8">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;ORCLoad&quot;,
    &quot;name&quot;: &quot;write customer records to orc&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputView&quot;: &quot;customer&quot;,
    &quot;outputURI&quot;: &quot;hdfs://datalake/raw/customer.orc&quot;,
    &quot;numPartitions&quot;: 100,
    &quot;partitionBy&quot;: [
        &quot;customer_segment&quot;,
        &quot;customer_type&quot;
    ],
    &quot;authentication&quot;: {
        ...
    },    
    &quot;saveMode&quot;: &quot;Append&quot;,
    &quot;params&quot;: {}
}
</code></pre>

<h2 id="parquetload">ParquetLoad</h2>

<h5 id="since-1-0-0-supports-streaming-true-4">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>ParquetLoad</code> writes an input <code>DataFrame</code> to a target <a href="https://parquet.apache.org/">Apache Parquet</a> file.</p>

<h3 id="parameters-9">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the Parquet file to write to.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The maximum number of partitions that can be used for parallelism in table reading and writing.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-9">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;ParquetLoad&quot;,
    &quot;name&quot;: &quot;write customer records to parquet&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputView&quot;: &quot;customer&quot;,
    &quot;outputURI&quot;: &quot;hdfs://datalake/raw/customer.parquet&quot;,
    &quot;numPartitions&quot;: 100,
    &quot;partitionBy&quot;: [
        &quot;customer_segment&quot;,
        &quot;customer_type&quot;
    ],
    &quot;authentication&quot;: {
        ...
    },    
    &quot;saveMode&quot;: &quot;Append&quot;,
    &quot;params&quot;: {}
}
</code></pre>

<h2 id="xmlload">XMLLoad</h2>

<h5 id="since-1-0-0-supports-streaming-false-3">Since: 1.0.0 - Supports Streaming: False</h5>

<p>The <code>XMLLoad</code> writes an input <code>DataFrame</code> to a target XML file.</p>

<h3 id="parameters-10">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the Parquet file to write to.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The maximum number of partitions that can be used for parallelism in table reading and writing.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-10">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;XMLLoad&quot;,
    &quot;name&quot;: &quot;write customer records to xml&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputView&quot;: &quot;customer&quot;,
    &quot;outputURI&quot;: &quot;hdfs://datalake/raw/customer.xml&quot;,
    &quot;numPartitions&quot;: 100,
    &quot;partitionBy&quot;: [
        &quot;customer_segment&quot;,
        &quot;customer_type&quot;
    ],
    &quot;authentication&quot;: {
        ...
    },    
    &quot;saveMode&quot;: &quot;Append&quot;,
    &quot;params&quot;: {}
}
</code></pre>


			<aside class="copyright" role="note">
				
				&copy; 2018 Released under the MIT license
				
			</aside>

			<footer class="footer">
				

<nav class="pagination" aria-label="Footer">
  <div class="previous">
  
      <a href="https://aglenergy.github.io/arc/transform/" title="Transform">
        <span class="direction">
          Previous
        </span>
        <div class="page">
          <div class="button button-previous" role="button" aria-label="Previous">
            <i class="icon icon-back"></i>
          </div>
          <div class="stretch">
            <div class="title">
              Transform
            </div>
          </div>
        </div>
      </a>
  
  </div>

  <div class="next">
  
      <a href="https://aglenergy.github.io/arc/execute/" title="Execute">
        <span class="direction">
          Next
        </span>
        <div class="page">
          <div class="stretch">
            <div class="title">
              Execute
            </div>
          </div>
          <div class="button button-next" role="button" aria-label="Next">
            <i class="icon icon-forward"></i>
          </div>
        </div>
      </a>
  
  </div>
</nav>





			</footer>
		</div>
	</article>

	<div class="results" role="status" aria-live="polite">
		<div class="scrollable">
			<div class="wrapper">
				<div class="meta"></div>
				<div class="list"></div>
			</div>
		</div>
	</div>
</main>

    <script>
    
      var base_url = 'https:\/\/aglenergy.github.io\/arc\/';
      var repo_id  = 'aglenergy\/arc';
    
    </script>

    <script src="https://aglenergy.github.io/arc/javascripts/application.js"></script>
    

    <script>
      /* Add headers to scrollspy */
      var headers   = document.getElementsByTagName("h2");
      var scrollspy = document.getElementById('scrollspy');

      if(scrollspy) {
        if(headers.length > 0) {
          for(var i = 0; i < headers.length; i++) {
            var li = document.createElement("li");
            li.setAttribute("class", "anchor");

            var a  = document.createElement("a");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", headers[i].innerHTML);
            a.innerHTML = headers[i].innerHTML;

            li.appendChild(a)
            scrollspy.appendChild(li);
          }
        } else {
          scrollspy.parentElement.removeChild(scrollspy)
        }


        /* Add permanent link next to the headers */
        var headers = document.querySelectorAll("h1, h2, h3, h4, h5, h6");

        for(var i = 0; i < headers.length; i++) {
            var a = document.createElement("a");
            a.setAttribute("class", "headerlink");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", "Permanent link")
            a.innerHTML = "#";
            headers[i].appendChild(a);
        }
      }
    </script>

    

    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/languages/scala.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </body>
</html>

