<!DOCTYPE html>
  
  
  
  
   <html class="no-js"> 

  <head lang="en-us">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=10" />
    <title>Load - Arc</title>
    <meta name="generator" content="Hugo 0.51" />

    
    <meta name="description" content="Arc is an opinionated framework for defining data pipelines which are predictable, repeatable and manageable.">
    
    <link rel="canonical" href="https://aglenergy.github.io/arc/load/">
    
    <meta name="author" content="au.com.agl.arc">
    

    <meta property="og:url" content="https://aglenergy.github.io/arc/load/">
    <meta property="og:title" content="Arc">
    <meta property="og:image" content="https://aglenergy.github.io/arc/images/logo.png">
    <meta name="apple-mobile-web-app-title" content="Arc">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <link rel="shortcut icon" type="image/x-icon" href="https://aglenergy.github.io/arc/images/favicon.ico">
    <link rel="icon" type="image/x-icon" href="https://aglenergy.github.io/arc/images/favicon.ico">

    <style>
      @font-face {
        font-family: 'Icon';
        src: url('https://aglenergy.github.io/arc/fonts/icon.eot');
        src: url('https://aglenergy.github.io/arc/fonts/icon.eot')
               format('embedded-opentype'),
             url('https://aglenergy.github.io/arc/fonts/icon.woff')
               format('woff'),
             url('https://aglenergy.github.io/arc/fonts/icon.ttf')
               format('truetype'),
             url('https://aglenergy.github.io/arc/fonts/icon.svg')
               format('svg');
        font-weight: normal;
        font-style: normal;
      }
    </style>

    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/application.css">
    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/temporary.css">
    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/palettes.css">
    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/highlight/highlight.css">

    
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ubuntu:400,700|Ubuntu&#43;Mono">
    <style>
      body, input {
        font-family: 'Ubuntu', Helvetica, Arial, sans-serif;
      }
      pre, code {
        font-family: 'Ubuntu Mono', 'Courier New', 'Courier', monospace;
      }
    </style>

    
    <script src="https://aglenergy.github.io/arc/javascripts/modernizr.js"></script>

    

  </head>
  <body class="palette-primary-red palette-accent-teal">



	
	


<div class="backdrop">
	<div class="backdrop-paper"></div>
</div>

<input class="toggle" type="checkbox" id="toggle-drawer">
<input class="toggle" type="checkbox" id="toggle-search">
<label class="toggle-button overlay" for="toggle-drawer"></label>

<header class="header">
	<nav aria-label="Header">
  <div class="bar default">
    <div class="button button-menu" role="button" aria-label="Menu">
      <label class="toggle-button icon icon-menu" for="toggle-drawer">
        <span></span>
      </label>
    </div>
    <div class="stretch">
      <div class="title">
        Load
      </div>
    </div>

    

    
    <div class="button button-github" role="button" aria-label="GitHub">
      <a href="https://github.com/aglenergy/arc" title="@https://github.com/aglenergy/arc on GitHub" target="_blank" class="toggle-button icon icon-github"></a>
    </div>
    
    
        
  </div>
  <div class="bar search">
    <div class="button button-close" role="button" aria-label="Close">
      <label class="toggle-button icon icon-back" for="toggle-search"></label>
    </div>
    <div class="stretch">
      <div class="field">
        <input class="query" type="text" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck>
      </div>
    </div>
    <div class="button button-reset" role="button" aria-label="Search">
      <button class="toggle-button icon icon-close" id="reset-search"></button>
    </div>
  </div>
</nav>
</header>

<main class="main">
	<div class="drawer">
		<nav aria-label="Navigation">
  <a href="https://aglenergy.github.io/arc/" class="project">
    <div class="banner">
      
      <div class="logo">
        <img src="https://aglenergy.github.io/arc/images/logo.png">
      </div>
      
      <div class="name">
        <strong>Arc 
          <span class="version">1.9.0</span></strong>
        
        <br> aglenergy/arc 
      </div>
    </div>
  </a>

  <div class="scrollable">
    <div class="wrapper">
      

      <div class="toc">
        
        <ul>
          




<li>
  
    



<a  title="Tutorial" href="https://aglenergy.github.io/arc/tutorial/">
	
	Tutorial
</a>



  
</li>



<li>
  
    



<a  title="Extract" href="https://aglenergy.github.io/arc/extract/">
	
	Extract
</a>



  
</li>



<li>
  
    



<a  title="Transform" href="https://aglenergy.github.io/arc/transform/">
	
	Transform
</a>



  
</li>



<li>
  
    



<a class="current" title="Load" href="https://aglenergy.github.io/arc/load/">
	
	Load
</a>


<ul id="scrollspy">
</ul>


  
</li>



<li>
  
    



<a  title="Execute" href="https://aglenergy.github.io/arc/execute/">
	
	Execute
</a>



  
</li>



<li>
  
    



<a  title="Validate" href="https://aglenergy.github.io/arc/validate/">
	
	Validate
</a>



  
</li>



<li>
  
    



<a  title="Metadata" href="https://aglenergy.github.io/arc/metadata/">
	
	Metadata
</a>



  
</li>



<li>
  
    



<a  title="Partials" href="https://aglenergy.github.io/arc/partials/">
	
	Partials
</a>



  
</li>



<li>
  
    



<a  title="Patterns" href="https://aglenergy.github.io/arc/patterns/">
	
	Patterns
</a>



  
</li>



<li>
  
    



<a  title="Deploy" href="https://aglenergy.github.io/arc/deploy/">
	
	Deploy
</a>



  
</li>



<li>
  
    



<a  title="Extend" href="https://aglenergy.github.io/arc/extend/">
	
	Extend
</a>



  
</li>



<li>
  
    



<a  title="Contributing" href="https://aglenergy.github.io/arc/contributing/">
	
	Contributing
</a>



  
</li>



<li>
  
    



<a  title="License" href="https://aglenergy.github.io/arc/license/">
	
	License
</a>



  
</li>


        </ul>
         
        <hr>
        <span class="section">The author</span>

        <ul>
           
          <li>
            <a href="https://github.com/aglenergy" target="_blank" title="@aglenergy on GitHub">
              @aglenergy on GitHub
            </a>
          </li>
           
        </ul>
        
      </div>
    </div>
  </div>
</nav>
	</div>

	<article class="article">
		<div class="wrapper">
			<h1>Load </h1>

			

<p><code>*Load</code> stages write out Spark <code>datasets</code> to a database or file system.</p>

<p><code>*Load</code> stages should meet this criteria:</p>

<ul>
<li>Take in a single <code>dataset</code>.</li>
<li>Perform target specific validation that the dataset has been written correctly.</li>
</ul>

<h2 id="avroload">AvroLoad</h2>

<h5 id="since-1-0-0-supports-streaming-false">Since: 1.0.0 - Supports Streaming: False</h5>

<p>The <code>AvroLoad</code> writes an input <code>DataFrame</code> to a target <a href="https://avro.apache.org/">Apache Avro</a> file.</p>

<h3 id="parameters">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the Avro file to write to.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>
</tbody>
</table>

<h3 id="examples">Examples</h3>

<h4 id="minimal">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;AvroLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer avro extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.avro&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;AvroLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer avro extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer avro extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.avro&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;saveMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Overwrite&#34;</span>
<span class="p">}</span></code></pre></div>

<h2 id="azureeventhubsload">AzureEventHubsLoad</h2>

<h5 id="since-1-0-0-supports-streaming-false-1">Since: 1.0.0 - Supports Streaming: False</h5>

<p>The <code>AzureEventHubsLoad</code> writes an input <code>DataFrame</code> to a target <a href="https://azure.microsoft.com/en-gb/services/event-hubs/">Azure Event Hubs</a> stream. The input to this stage needs to be a single column dataset of signature <code>value: string</code> and is intended to be used after a <a href="https://aglenergy.github.io/arc/load/#jsontransform">JSONTransform</a> stage which would prepare the data for sending to the external server.</p>

<p>In the future additional Transform stages (like <code>ProtoBufTransform</code>) could be added to prepare binary payloads instead of just <code>json</code> <code>string</code>.</p>

<h3 id="parameters-1">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>namespaceName</td>
<td>String</td>
<td>true</td>
<td>The Event Hubs namespace.</td>
</tr>

<tr>
<td>eventHubName</td>
<td>String</td>
<td>true</td>
<td>The Event Hubs entity.</td>
</tr>

<tr>
<td>sharedAccessSignatureKeyName</td>
<td>String</td>
<td>true</td>
<td>The Event Hubs Shared Access Signature Key Name.</td>
</tr>

<tr>
<td>sharedAccessSignatureKey</td>
<td>String</td>
<td>true</td>
<td>The Event Hubs Shared Access Signature Key.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism. Azure EventHubs will throw a <code>ServerBusyException</code> if too many executors write to a target in parallel which can be decreased by reducing the number of partitions.</td>
</tr>

<tr>
<td>retryCount</td>
<td>Integer</td>
<td>false</td>
<td>The maximum number of retries for the exponential backoff algorithm.<br><br>Default: 10.</td>
</tr>

<tr>
<td>retryMaxBackoff</td>
<td>Long</td>
<td>false</td>
<td>The maximum time (in seconds) for the exponential backoff algorithm to wait between retries.<br><br>Default: 30.</td>
</tr>

<tr>
<td>retryMinBackoff</td>
<td>Long</td>
<td>false</td>
<td>The minimum time (in seconds) for the exponential backoff algorithm to wait between retries.<br><br>Default: 0.</td>
</tr>
</tbody>
</table>

<h3 id="examples-1">Examples</h3>

<h4 id="minimal-1">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;AzureEventHubsLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer to azure event hubs&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;namespaceName&#34;</span><span class="p">:</span> <span class="s2">&#34;mynamespace&#34;</span><span class="p">,</span>
  <span class="nt">&#34;eventHubName&#34;</span><span class="p">:</span> <span class="s2">&#34;myeventhub&#34;</span><span class="p">,</span>
  <span class="nt">&#34;sharedAccessSignatureKeyName&#34;</span><span class="p">:</span> <span class="s2">&#34;mysignaturename&#34;</span><span class="p">,</span>
  <span class="nt">&#34;sharedAccessSignatureKey&#34;</span><span class="p">:</span> <span class="s2">&#34;ctzMq410TV3wS7upTBcunJTDLEJwMAZuFPfr0mrrA08=&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-1">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;AzureEventHubsLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer to azure event hubs&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer to azure event hubs&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;namespaceName&#34;</span><span class="p">:</span> <span class="s2">&#34;mynamespace&#34;</span><span class="p">,</span>
  <span class="nt">&#34;eventHubName&#34;</span><span class="p">:</span> <span class="s2">&#34;myeventhub&#34;</span><span class="p">,</span>
  <span class="nt">&#34;sharedAccessSignatureKeyName&#34;</span><span class="p">:</span> <span class="s2">&#34;mysignaturename&#34;</span><span class="p">,</span>
  <span class="nt">&#34;sharedAccessSignatureKey&#34;</span><span class="p">:</span> <span class="s2">&#34;ctzMq410TV3wS7upTBcunJTDLEJwMAZuFPfr0mrrA08=&#34;</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
  <span class="nt">&#34;retryCount&#34;</span><span class="p">:</span> <span class="mi">30</span><span class="p">,</span>
  <span class="nt">&#34;retryMaxBackoff&#34;</span><span class="p">:</span> <span class="mi">60</span><span class="p">,</span>
  <span class="nt">&#34;retryMinBackoff&#34;</span><span class="p">:</span> <span class="mi">5</span>
<span class="p">}</span></code></pre></div>

<h2 id="consoleload">ConsoleLoad</h2>

<h5 id="since-1-2-0-supports-streaming-true">Since: 1.2.0 - Supports Streaming: True</h5>

<p>The <code>ConsoleLoad</code> prints an input streaming <code>DataFrame</code> the console.</p>

<p>This stage has been included for testing Structured Streaming jobs as it can be very difficult to debug. Generally this stage would only be included when Arc is run in a test mode (i.e. the <code>environment</code> is set to <code>test</code>).</p>

<h3 id="parameters-2">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>outputMode</td>
<td>String</td>
<td>false</td>
<td>The output mode of the console writer. Allowed values <code>Append</code>, <code>Complete</code>, <code>Update</code>. See <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes">Output Modes</a> for full details.<br><br>Default: <code>Append</code></td>
</tr>
</tbody>
</table>

<h3 id="examples-2">Examples</h3>

<h4 id="minimal-2">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ConsoleLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write a streaming dataset to console&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-2">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ConsoleLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write a streaming dataset to console&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;write a streaming dataset to console&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Append&#34;</span>
<span class="p">}</span></code></pre></div>

<h2 id="databricksdeltaload">DatabricksDeltaLoad</h2>

<h5 id="since-1-8-0-supports-streaming-true">Since: 1.8.0 - Supports Streaming: True</h5>

<div class="admonition note">
<p class="admonition-title">Experimental</p>
<p><p>The <code>DatabricksDeltaLoad</code> is currently in experimental state whilst the requirements become clearer.</p>

<p>This means this API is likely to change.</p>
</p>
</div>

<p>The <code>DatabricksDeltaLoad</code> writes an input <code>DataFrame</code> to a target <a href="https://databricks.com/product/databricks-delta/">Databricks Delta</a> file.</p>

<h3 id="parameters-3">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the Delta file to write to.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>
</tbody>
</table>

<h3 id="examples-3">Examples</h3>

<h4 id="minimal-3">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;DatabricksDeltaLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer Delta extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;/delta/customers&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-3">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;DatabricksDeltaLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer Delta extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer Delta extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;/delta/customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;saveMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Overwrite&#34;</span>
<span class="p">}</span></code></pre></div>

<h2 id="databrickssqldwload">DatabricksSQLDWLoad</h2>

<h5 id="since-1-8-1-supports-streaming-false">Since: 1.8.1 - Supports Streaming: False</h5>

<div class="admonition note">
<p class="admonition-title">Experimental</p>
<p><p>The <code>DatabricksSQLDWLoad</code> is currently in experimental state whilst the requirements become clearer.</p>

<p>This means this API is likely to change.</p>
</p>
</div>

<p>The <code>DatabricksSQLDWLoad</code> writes an input <code>DataFrame</code> to a target <a href="https://azure.microsoft.com/en-au/services/sql-data-warehouse/">Azure SQL Data Warehouse</a> file using the proprietary driver within a Databricks Runtime Environment.</p>

<p>Known limitations:</p>

<ul>
<li>SQL Server date fields can only be between range <code>1753-01-01</code> to <code>9999-12-31</code>.</li>
</ul>

<h3 id="parameters-4">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>jdbcURL</td>
<td>URI</td>
<td>true</td>
<td>URI of the Delta file to write to.</td>
</tr>

<tr>
<td>dbTable</td>
<td>String</td>
<td>true</td>
<td>The table to create in SQL DW.</td>
</tr>

<tr>
<td>tempDir</td>
<td>URI</td>
<td>true</td>
<td>A Azure Blob Storage path to temporarily hold the data before executing the SQLDW load.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>true</td>
<td>An authentication map for authenticating with a remote service. See <a href="../partials/#authentication">authentication</a> documentation.. Note this stage only works with the <code>AzureSharedKey</code> <a href="../partials/#authentication">authentication</a> method.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>forwardSparkAzureStorageCredentials</td>
<td>Boolean</td>
<td>false</td>
<td>If true, the library automatically discovers the credentials that Spark is using to connect to the Blob Storage container and forwards those credentials to SQL DW over JDBC.<br><br>Default: <code>true</code>.</td>
</tr>

<tr>
<td>tableOptions</td>
<td>String</td>
<td>false</td>
<td>Used to specify table options when creating the SQL DW table.</td>
</tr>

<tr>
<td>maxStrLength</td>
<td>Integer</td>
<td>false</td>
<td>The default length of <code>String</code>/<code>NVARCHAR</code> columns when creating the table in SQLDW.<br><br>Default: <code>256</code>.</td>
</tr>
</tbody>
</table>

<h3 id="examples-4">Examples</h3>

<h4 id="minimal-4">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;DatabricksSQLDWLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;method&#34;</span><span class="p">:</span> <span class="s2">&#34;AzureSharedKey&#34;</span><span class="p">,</span>
    <span class="nt">&#34;accountName&#34;</span><span class="p">:</span> <span class="s2">&#34;myaccount&#34;</span><span class="p">,</span>
    <span class="nt">&#34;signature&#34;</span><span class="p">:</span> <span class="s2">&#34;ctzMq410TV3wS7upTBcunJTDLEJwMAZuFPfr0mrrA08=&#34;</span>
  <span class="p">},</span>
  <span class="nt">&#34;jdbcURL&#34;</span><span class="p">:</span> <span class="s2">&#34;jdbc:sqlserver://localhost;user=MyUserName;password=*****&#34;</span><span class="p">,</span>
  <span class="nt">&#34;dbTable&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;tempDir&#34;</span><span class="p">:</span> <span class="s2">&#34;wasbs://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.blob.core.windows.net/&lt;your-directory-name&gt;&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-4">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;DatabricksSQLDWLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;method&#34;</span><span class="p">:</span> <span class="s2">&#34;AzureSharedKey&#34;</span><span class="p">,</span>
    <span class="nt">&#34;accountName&#34;</span><span class="p">:</span> <span class="s2">&#34;myaccount&#34;</span><span class="p">,</span>
    <span class="nt">&#34;signature&#34;</span><span class="p">:</span> <span class="s2">&#34;ctzMq410TV3wS7upTBcunJTDLEJwMAZuFPfr0mrrA08=&#34;</span>
  <span class="p">},</span>  
  <span class="nt">&#34;jdbcURL&#34;</span><span class="p">:</span> <span class="s2">&#34;jdbc:sqlserver://localhost;user=MyUserName;password=*****&#34;</span><span class="p">,</span>
  <span class="nt">&#34;dbTable&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;tempDir&#34;</span><span class="p">:</span> <span class="s2">&#34;wasbs://&lt;your-container-name&gt;@&lt;your-storage-account-name&gt;.blob.core.windows.net/&lt;your-directory-name&gt;&#34;</span><span class="p">,</span>
  <span class="nt">&#34;forwardSparkAzureStorageCredentials&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span> 
  <span class="nt">&#34;tableOptions&#34;</span><span class="p">:</span> <span class="s2">&#34;CLUSTERED COLUMNSTORE INDEX, DISTRIBUTION = ROUND_ROBIN&#34;</span><span class="p">,</span> 
  <span class="nt">&#34;maxStrLength&#34;</span><span class="p">:</span> <span class="mi">1024</span>
<span class="p">}</span></code></pre></div>

<h2 id="delimitedload">DelimitedLoad</h2>

<h5 id="since-1-0-0-supports-streaming-true">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>DelimitedLoad</code> writes an input <code>DataFrame</code> to a target delimited file.</p>

<h3 id="parameters-5">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the Delimited file to write to.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>customDelimiter</td>
<td>String</td>
<td>true*</td>
<td>A custom string to use as delimiter. Required if <code>delimiter</code> is set to <code>Custom</code>.</td>
</tr>

<tr>
<td>delimiter</td>
<td>String</td>
<td>false</td>
<td>The type of delimiter in the file. Supported values: <code>Comma</code>, <code>Pipe</code>, <code>DefaultHive</code>. <code>DefaultHive</code> is  ASCII character 1, the default delimiter for Apache Hive extracts.<br><br>Default: <code>Comma</code>.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>header</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to write a header row.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>quote</td>
<td>String</td>
<td>false</td>
<td>The type of quoting in the file. Supported values: <code>None</code>, <code>SingleQuote</code>, <code>DoubleQuote</code>.<br><br>Default: <code>DoubleQuote</code>.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>
</tbody>
</table>

<h3 id="examples-5">Examples</h3>

<h4 id="minimal-5">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;DelimitedLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer as csv&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.csv&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-5">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;DelimitedLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer as csv&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer as csv&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.csv&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;delimiter&#34;</span><span class="p">:</span> <span class="s2">&#34;Custom&#34;</span><span class="p">,</span>
  <span class="nt">&#34;customDelimiter&#34;</span><span class="p">:</span> <span class="s2">&#34;#&#34;</span><span class="p">,</span>
  <span class="nt">&#34;header&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;quote&#34;</span><span class="p">:</span> <span class="s2">&#34;DoubleQuote&#34;</span><span class="p">,</span>
  <span class="nt">&#34;saveMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Overwrite&#34;</span>
<span class="p">}</span></code></pre></div>

<h2 id="httpload">HTTPLoad</h2>

<h5 id="since-1-0-0-supports-streaming-false-2">Since: 1.0.0 - Supports Streaming: False</h5>

<p>The <code>HTTPLoad</code> takes an input <code>DataFrame</code> and executes a series of <code>POST</code> requests against a remote HTTP service. The input to this stage needs to be a single column dataset of signature <code>value: string</code> and is intended to be used after a <a href="https://aglenergy.github.io/arc/load/#jsontransform">JSONTransform</a> stage which would prepare the data for sending to the external server.</p>

<p>In the future additional Transform stages (like <code>ProtoBufTransform</code>) could be added to prepare binary payloads instead of just <code>json</code> <code>string</code>.</p>

<h3 id="parameters-6">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the HTTP server.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>headers</td>
<td>Map[String, String]</td>
<td>false</td>
<td><a href="https://en.wikipedia.org/wiki/List_of_HTTP_header_fields">HTTP Headers</a> to set for the HTTP request. These are not limited to the Internet Engineering Task Force standard headers.</td>
</tr>

<tr>
<td>validStatusCodes</td>
<td>Array[Integer]</td>
<td>false</td>
<td>A list of valid status codes which will result in a successful stage if the list contains the HTTP server response code. If not provided the default values are <code>[200, 201, 202]</code>. Note: all request response codes must be contained in this list for the stage to be successful.</td>
</tr>
</tbody>
</table>

<h3 id="examples-6">Examples</h3>

<h4 id="minimal-6">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;HTTPLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers to the customer api&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;http://internalserver/api/customer&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-6">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;HTTPLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers to the customer api&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers to the customer api&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;http://internalserver/api/customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;headers&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;Authorization&#34;</span><span class="p">:</span> <span class="s2">&#34;Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==&#34;</span><span class="p">,</span>
    <span class="nt">&#34;custom-header&#34;</span><span class="p">:</span> <span class="s2">&#34;payload&#34;</span>
  <span class="p">},</span>
  <span class="nt">&#34;validStatusCodes&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="mi">200</span><span class="p">,</span>
    <span class="mi">201</span>
  <span class="p">]</span>
<span class="p">}</span></code></pre></div>

<h2 id="jdbcload">JDBCLoad</h2>

<h5 id="since-1-0-0-supports-streaming-true-1">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>JDBCLoad</code> writes an input <code>DataFrame</code> to a target JDBC Database. See <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#jdbc-to-other-databases">Spark JDBC documentation</a>.</p>

<p>Whilst it is possible to use <code>JDBCLoad</code> to create tables directly in the target database Spark only has a limited knowledge of the schema required in the destination database and so will translate things like <code>StringType</code> internally to a <code>TEXT</code> type in the target database (because internally Spark does not have limited length strings). The recommendation is to use a preceding <a href="../execute/#jdbcexecute">JDBCExecute</a> to execute a <code>CREATE TABLE</code> statement which creates the intended schema then inserting into that table with <code>saveMode</code> set to <code>Append</code>.</p>

<h3 id="parameters-7">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>jdbcURL</td>
<td>String</td>
<td>true</td>
<td>The JDBC URL to connect to. e.g., <code>jdbc:mysql://localhost:3306</code>.</td>
</tr>

<tr>
<td>tableName</td>
<td>String</td>
<td>true</td>
<td>The target JDBC table. Must be in <code>database</code>.<code>schema</code>.<code>table</code> format.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>true</td>
<td>Map of configuration parameters.. Currently requires <code>user</code> and <code>password</code> to be set here - see example below.</td>
</tr>

<tr>
<td>batchsize</td>
<td>Integer</td>
<td>false</td>
<td>The JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers.<br><br>Default: <code>1000</code>.</td>
</tr>

<tr>
<td>bulkload</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to enable a <code>bulk</code> copy. This is currently only available for <code>sqlserver</code> targets but more targets can be added as drivers become available.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>createTableColumnTypes</td>
<td>String</td>
<td>false</td>
<td>The database column data types to use instead of the defaults, when creating the table. Data type information should be specified in the same format as <code>CREATE TABLE</code> columns syntax (e.g: &ldquo;<code>name CHAR(64), comments VARCHAR(1024)</code>&rdquo;). The specified types should be valid spark sql data types.</td>
</tr>

<tr>
<td>createTableOptions</td>
<td>String</td>
<td>false</td>
<td>This is a JDBC writer related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g., <code>CREATE TABLE t (name string) ENGINE=InnoDB</code>).</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>isolationLevel</td>
<td>String</td>
<td>false</td>
<td>The transaction isolation level, which applies to current connection. It can be one of NONE, READ_COMMITTED, READ_UNCOMMITTED, REPEATABLE_READ, or SERIALIZABLE, corresponding to standard transaction isolation levels defined by JDBC&rsquo;s Connection object, with default of READ_UNCOMMITTED. Please refer the documentation in <a href="https://docs.oracle.com/javase/8/docs/api/java/sql/Connection.html">java.sql.Connection</a>.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism. This also determines the maximum number of concurrent JDBC connections.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>

<tr>
<td>tablock</td>
<td>Boolean</td>
<td>false</td>
<td>When in <code>bulkload</code> mode whether to set <code>TABLOCK</code> on the driver.<br><br>Default: <code>true</code>.</td>
</tr>

<tr>
<td>truncate</td>
<td>Boolean</td>
<td>false</td>
<td>If using <code>SaveMode</code> equal to <code>Overwrite</code>, this additional option causes Spark to <code>TRUNCATE TABLE</code> of existing data instead of executing a <code>DELETE FROM</code> statement.</td>
</tr>
</tbody>
</table>

<h3 id="examples-7">Examples</h3>

<h4 id="minimal-7">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;JDBCLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer to postgres&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;jdbcURL&#34;</span><span class="p">:</span> <span class="s2">&#34;jdbc:postgresql://localhost:5432/customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;tableName&#34;</span><span class="p">:</span> <span class="s2">&#34;mydatabase.myschema.customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;params&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;user&#34;</span><span class="p">:</span> <span class="s2">&#34;mydbuser&#34;</span><span class="p">,</span>
    <span class="nt">&#34;password&#34;</span><span class="p">:</span> <span class="s2">&#34;mydbpassword&#34;</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-7">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;JDBCLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer to postgres&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer to postgres&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;jdbcURL&#34;</span><span class="p">:</span> <span class="s2">&#34;jdbc:postgresql://localhost:5432/customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;tableName&#34;</span><span class="p">:</span> <span class="s2">&#34;mydatabase.myschema.customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;batchsize&#34;</span><span class="p">:</span> <span class="mi">10000</span><span class="p">,</span>
  <span class="nt">&#34;bulkload&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;createTableColumnTypes&#34;</span><span class="p">:</span> <span class="s2">&#34;name CHAR(64), comments VARCHAR(1024)&#34;</span><span class="p">,</span>
  <span class="nt">&#34;createTableOptions&#34;</span><span class="p">:</span> <span class="s2">&#34;CREATE TABLE t (name string) ENGINE=InnoDB&#34;</span><span class="p">,</span>
  <span class="nt">&#34;isolationLevel&#34;</span><span class="p">:</span> <span class="s2">&#34;READ_COMMITTED&#34;</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;params&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;user&#34;</span><span class="p">:</span> <span class="s2">&#34;mydbuser&#34;</span><span class="p">,</span>
    <span class="nt">&#34;password&#34;</span><span class="p">:</span> <span class="s2">&#34;mydbpassword&#34;</span>
  <span class="p">},</span>
  <span class="nt">&#34;saveMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Append&#34;</span><span class="p">,</span>
  <span class="nt">&#34;tablock&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;truncate&#34;</span><span class="p">:</span> <span class="kc">false</span>
<span class="p">}</span></code></pre></div>

<h2 id="jsonload">JSONLoad</h2>

<h5 id="since-1-0-0-supports-streaming-true-2">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>JSONLoad</code> writes an input <code>DataFrame</code> to a target JSON file.</p>

<h3 id="parameters-8">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the Delimited file to write to.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>
</tbody>
</table>

<h3 id="examples-8">Examples</h3>

<h4 id="minimal-8">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;JSONLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer json extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.json&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-8">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;JSONLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer json extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer json extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.json&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;saveMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Overwrite&#34;</span>
<span class="p">}</span></code></pre></div>

<h2 id="kafkaload">KafkaLoad</h2>

<h5 id="since-1-0-8-supports-streaming-true">Since: 1.0.8 - Supports Streaming: True</h5>

<p>The <code>KafkaLoad</code> writes an input <code>DataFrame</code> to a target <a href="https://kafka.apache.org/">Kafka</a> <code>topic</code>. The input to this stage needs to be a single column dataset of signature <code>value: string</code> - intended to be used after a <a href="https://aglenergy.github.io/arc/load/#jsontransform">JSONTransform</a> stage - or a two columns of signature <code>key: string, value: string</code> which could be created by a <a href="https://aglenergy.github.io/arc/load/#sqltransform">SQLTransform</a> stage.</p>

<p>In the future additional Transform stages (like <code>ProtoBufTransform</code>) may be added to prepare binary payloads instead of just <code>json</code> <code>string</code>.</p>

<h3 id="parameters-9">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>bootstrapServers</td>
<td>String</td>
<td>true</td>
<td>A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. e.g. <code>host1:port1,host2:port2,...</code></td>
</tr>

<tr>
<td>topic</td>
<td>String</td>
<td>true</td>
<td>The target Kafka topic.</td>
</tr>

<tr>
<td>acks</td>
<td>Integer</td>
<td>false</td>
<td>The number of acknowledgments the producer requires the leader to have received before considering a request complete.<br><br>Alowed values:<br><code>1</code>: the leader will write the record to its local log but will respond without awaiting full acknowledgement from all followers.<br><code>0</code>:  the job will not wait for any acknowledgment from the server at all.<br><code>-1</code>:  the leader will wait for the full set of in-sync replicas to acknowledge the record (safest).<br><br>Default: <code>1</code>.</td>
</tr>

<tr>
<td>batchSize</td>
<td>Integer</td>
<td>false</td>
<td>Number of records to send in single requet to reduce number of requests to Kafka.<br><br>Default: <code>16384</code>.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>retries</td>
<td>Integer</td>
<td>false</td>
<td>How many times to try to resend any record whose send fails with a potentially transient error.<br><br>Default: <code>0</code>.</td>
</tr>
</tbody>
</table>

<h3 id="examples-9">Examples</h3>

<h4 id="minimal-9">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;KafkaLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer to kafka&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;bootstrapServers&#34;</span><span class="p">:</span> <span class="s2">&#34;kafka:29092&#34;</span><span class="p">,</span>
  <span class="nt">&#34;topic&#34;</span><span class="p">:</span> <span class="s2">&#34;customers&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-9">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;KafkaLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer to kafka&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer to kafka&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;bootstrapServers&#34;</span><span class="p">:</span> <span class="s2">&#34;kafka:29092&#34;</span><span class="p">,</span>
  <span class="nt">&#34;topic&#34;</span><span class="p">:</span> <span class="s2">&#34;customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;acks&#34;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
  <span class="nt">&#34;batchSize&#34;</span><span class="p">:</span> <span class="mi">16384</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;retries&#34;</span><span class="p">:</span> <span class="mi">0</span>
<span class="p">}</span></code></pre></div>

<h2 id="orcload">ORCLoad</h2>

<h5 id="since-1-0-0-supports-streaming-true-3">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>ORCLoad</code> writes an input <code>DataFrame</code> to a target <a href="https://orc.apache.org/">Apache ORC</a> file.</p>

<h3 id="parameters-10">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the ORC file to write to.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>
</tbody>
</table>

<h3 id="examples-10">Examples</h3>

<h4 id="minimal-10">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ORCLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer ORC extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.orc&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-10">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ORCLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer ORC extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer ORC extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.orc&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;saveMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Overwrite&#34;</span>
<span class="p">}</span></code></pre></div>

<h2 id="parquetload">ParquetLoad</h2>

<h5 id="since-1-0-0-supports-streaming-true-4">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>ParquetLoad</code> writes an input <code>DataFrame</code> to a target <a href="https://parquet.apache.org/">Apache Parquet</a> file.</p>

<h3 id="parameters-11">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the Parquet file to write to.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>
</tbody>
</table>

<h3 id="examples-11">Examples</h3>

<h4 id="minimal-11">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ParquetLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer Parquet extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.parquet&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-11">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ParquetLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer Parquet extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer Parquet extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.parquet&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;saveMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Overwrite&#34;</span>
<span class="p">}</span></code></pre></div>

<h2 id="xmlload">XMLLoad</h2>

<h5 id="since-1-0-0-supports-streaming-false-3">Since: 1.0.0 - Supports Streaming: False</h5>

<p>The <code>XMLLoad</code> writes an input <code>DataFrame</code> to a target XML file.</p>

<h3 id="parameters-12">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the XML file to write to.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>
</tbody>
</table>

<h3 id="examples-12">Examples</h3>

<h4 id="minimal-12">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;XMLLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer XML extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.xml&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-12">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;XMLLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer XML extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer XML extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.xml&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;saveMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Overwrite&#34;</span>
<span class="p">}</span></code></pre></div>


			<aside class="copyright" role="note">
				
				&copy; 2019 Released under the MIT license
				
			</aside>

			<footer class="footer">
				

<nav class="pagination" aria-label="Footer">
  <div class="previous">
    
    <a href="https://aglenergy.github.io/arc/transform/" title="Transform">
      <span class="direction">
        Previous
      </span>
      <div class="page">
        <div class="button button-previous" role="button" aria-label="Previous">
          <i class="icon icon-back"></i>
        </div>
        <div class="stretch">
          <div class="title">
            Transform
          </div>
        </div>
      </div>
    </a>
    
  </div>

  <div class="next">
    
    <a href="https://aglenergy.github.io/arc/execute/" title="Execute">
      <span class="direction">
        Next
      </span>
      <div class="page">
        <div class="stretch">
          <div class="title">
            Execute
          </div>
        </div>
        <div class="button button-next" role="button" aria-label="Next">
          <i class="icon icon-forward"></i>
        </div>
      </div>
    </a>
    
  </div>
</nav>




			</footer>
		</div>
	</article>

	<div class="results" role="status" aria-live="polite">
		<div class="scrollable">
			<div class="wrapper">
				<div class="meta"></div>
				<div class="list"></div>
			</div>
		</div>
	</div>
</main>

    <script>
    
      var base_url = 'https:\/\/aglenergy.github.io\/arc\/';
      var repo_id  = 'aglenergy\/arc';
    
    </script>

    <script src="https://aglenergy.github.io/arc/javascripts/application.js"></script>
    

    <script>
      /* Add headers to scrollspy */
      var headers   = document.getElementsByTagName("h2");
      var scrollspy = document.getElementById('scrollspy');

      if(scrollspy) {
        if(headers.length > 0) {
          for(var i = 0; i < headers.length; i++) {
            var li = document.createElement("li");
            li.setAttribute("class", "anchor");

            var a  = document.createElement("a");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", headers[i].innerHTML);
            a.innerHTML = headers[i].innerHTML;

            li.appendChild(a)
            scrollspy.appendChild(li);
          }
        } else {
          scrollspy.parentElement.removeChild(scrollspy)
        }


        /* Add permanent link next to the headers */
        var headers = document.querySelectorAll("h1, h2, h3, h4, h5, h6");

        for(var i = 0; i < headers.length; i++) {
            var a = document.createElement("a");
            a.setAttribute("class", "headerlink");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", "Permanent link")
            a.innerHTML = "#";
            headers[i].appendChild(a);
        }
      }
    </script>

    

    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/languages/scala.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </body>
</html>

