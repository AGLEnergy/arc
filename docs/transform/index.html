<!DOCTYPE html>
  
  
  
  
   <html class="no-js"> 

  <head lang="en-us">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=10" />
    <title>Transform - Arc</title>
    <meta name="generator" content="Hugo 0.41" />

    
    <meta name="description" content="Arc is an opinionated framework for defining data pipelines which are predictable, repeatable and manageable.">
    
    <link rel="canonical" href="https://aglenergy.github.io/arc/transform/">
    
    <meta name="author" content="au.com.agl.arc">
    

    <meta property="og:url" content="https://aglenergy.github.io/arc/transform/">
    <meta property="og:title" content="Arc">
    <meta property="og:image" content="https://aglenergy.github.io/arc/images/logo.png">
    <meta name="apple-mobile-web-app-title" content="Arc">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <link rel="shortcut icon" type="image/x-icon" href="https://aglenergy.github.io/arc/images/favicon.ico">
    <link rel="icon" type="image/x-icon" href="https://aglenergy.github.io/arc/images/favicon.ico">

    <style>
      @font-face {
        font-family: 'Icon';
        src: url('https://aglenergy.github.io/arc/fonts/icon.eot');
        src: url('https://aglenergy.github.io/arc/fonts/icon.eot')
               format('embedded-opentype'),
             url('https://aglenergy.github.io/arc/fonts/icon.woff')
               format('woff'),
             url('https://aglenergy.github.io/arc/fonts/icon.ttf')
               format('truetype'),
             url('https://aglenergy.github.io/arc/fonts/icon.svg')
               format('svg');
        font-weight: normal;
        font-style: normal;
      }
    </style>

    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/application.css">
    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/temporary.css">
    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/palettes.css">
    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/highlight/highlight.css">

    
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ubuntu:400,700|Ubuntu&#43;Mono">
    <style>
      body, input {
        font-family: 'Ubuntu', Helvetica, Arial, sans-serif;
      }
      pre, code {
        font-family: 'Ubuntu Mono', 'Courier New', 'Courier', monospace;
      }
    </style>

    
    <script src="https://aglenergy.github.io/arc/javascripts/modernizr.js"></script>

    

  </head>
  <body class="palette-primary-red palette-accent-teal">



	
	


<div class="backdrop">
	<div class="backdrop-paper"></div>
</div>

<input class="toggle" type="checkbox" id="toggle-drawer">
<input class="toggle" type="checkbox" id="toggle-search">
<label class="toggle-button overlay" for="toggle-drawer"></label>

<header class="header">
	<nav aria-label="Header">
  <div class="bar default">
    <div class="button button-menu" role="button" aria-label="Menu">
      <label class="toggle-button icon icon-menu" for="toggle-drawer">
        <span></span>
      </label>
    </div>
    <div class="stretch">
      <div class="title">
        Transform
      </div>
    </div>

    

    
    <div class="button button-github" role="button" aria-label="GitHub">
      <a href="https://github.com/aglenergy/arc" title="@https://github.com/aglenergy/arc on GitHub" target="_blank" class="toggle-button icon icon-github"></a>
    </div>
    
    
        
  </div>
  <div class="bar search">
    <div class="button button-close" role="button" aria-label="Close">
      <label class="toggle-button icon icon-back" for="toggle-search"></label>
    </div>
    <div class="stretch">
      <div class="field">
        <input class="query" type="text" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck>
      </div>
    </div>
    <div class="button button-reset" role="button" aria-label="Search">
      <button class="toggle-button icon icon-close" id="reset-search"></button>
    </div>
  </div>
</nav>
</header>

<main class="main">
	<div class="drawer">
		<nav aria-label="Navigation">
  <a href="https://aglenergy.github.io/arc/" class="project">
    <div class="banner">
      
      <div class="logo">
        <img src="https://aglenergy.github.io/arc/images/logo.png">
      </div>
      
      <div class="name">
        <strong>Arc 
          <span class="version">1.3.0</span></strong>
        
        <br> aglenergy/arc 
      </div>
    </div>
  </a>

  <div class="scrollable">
    <div class="wrapper">
      

      <div class="toc">
        
        <ul>
          




<li>
  
    



<a  title="Tutorial" href="https://aglenergy.github.io/arc/tutorial/">
	
	Tutorial
</a>



  
</li>



<li>
  
    



<a  title="Extract" href="https://aglenergy.github.io/arc/extract/">
	
	Extract
</a>



  
</li>



<li>
  
    



<a class="current" title="Transform" href="https://aglenergy.github.io/arc/transform/">
	
	Transform
</a>


<ul id="scrollspy">
</ul>


  
</li>



<li>
  
    



<a  title="Load" href="https://aglenergy.github.io/arc/load/">
	
	Load
</a>



  
</li>



<li>
  
    



<a  title="Execute" href="https://aglenergy.github.io/arc/execute/">
	
	Execute
</a>



  
</li>



<li>
  
    



<a  title="Validate" href="https://aglenergy.github.io/arc/validate/">
	
	Validate
</a>



  
</li>



<li>
  
    



<a  title="Metadata" href="https://aglenergy.github.io/arc/metadata/">
	
	Metadata
</a>



  
</li>



<li>
  
    



<a  title="Partials" href="https://aglenergy.github.io/arc/partials/">
	
	Partials
</a>



  
</li>



<li>
  
    



<a  title="Patterns" href="https://aglenergy.github.io/arc/patterns/">
	
	Patterns
</a>



  
</li>



<li>
  
    



<a  title="Deploy" href="https://aglenergy.github.io/arc/deploy/">
	
	Deploy
</a>



  
</li>



<li>
  
    



<a  title="Extend" href="https://aglenergy.github.io/arc/extend/">
	
	Extend
</a>



  
</li>



<li>
  
    



<a  title="Contributing" href="https://aglenergy.github.io/arc/contributing/">
	
	Contributing
</a>



  
</li>



<li>
  
    



<a  title="License" href="https://aglenergy.github.io/arc/license/">
	
	License
</a>



  
</li>


        </ul>
         
        <hr>
        <span class="section">The author</span>

        <ul>
           
          <li>
            <a href="https://github.com/aglenergy" target="_blank" title="@aglenergy on GitHub">
              @aglenergy on GitHub
            </a>
          </li>
           
        </ul>
        
      </div>
    </div>
  </div>
</nav>
	</div>

	<article class="article">
		<div class="wrapper">
			<h1>Transform </h1>

			

<p><code>*Transform</code> stages apply a single transformation to one or more incoming datasets.</p>

<p>Transformers should meet this criteria:</p>

<ul>
<li>Be <a href="https://en.wikipedia.org/wiki/Pure_function">pure</a>.</li>
<li>Perform only a <a href="https://en.wikipedia.org/wiki/Separation_of_concerns">single function</a>.</li>
<li>Utilise Spark <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">internal functionality</a> where possible.</li>
</ul>

<h2 id="difftransform">DiffTransform</h2>

<h5 id="since-1-0-8-supports-streaming-false">Since: 1.0.8 - Supports Streaming: False</h5>

<p>The <code>DiffTransform</code> stage calculates the difference between two input datasets and produces three datasets:</p>

<ul>
<li>A dataset of the <code>intersection</code> of the two datasets - or rows that exist and are the same in both datasets.</li>
<li>A dataset of the <code>left</code> dataset - or rows that only exist in the left input dataset (<code>inputLeftView</code>).</li>
<li>A dataset of the <code>right</code> dataset - or rows that only exist in the right input dataset (<code>inputRightView</code>).</li>
</ul>

<div class="admonition note">
<p class="admonition-title">Persistence</p>
<p>This stage performs this &lsquo;diffing&rsquo; operation in a single pass so if multiple of the output views are going to be used then it is a good idea to set persist = <code>true</code> to reduce the cost of recomputing the difference multiple times.</p>
</div>

<h3 id="parameters">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>inputLeftView</td>
<td>String</td>
<td>true</td>
<td>Name of first incoming Spark dataset.</td>
</tr>

<tr>
<td>inputRightView</td>
<td>String</td>
<td>true</td>
<td>Name of second incoming Spark dataset.</td>
</tr>

<tr>
<td>outputIntersectionView</td>
<td>String</td>
<td>false</td>
<td>Name of output <code>intersection</code> view.</td>
</tr>

<tr>
<td>outputLeftView</td>
<td>String</td>
<td>false</td>
<td>Name of output <code>left</code> view.</td>
</tr>

<tr>
<td>outputRightView</td>
<td>String</td>
<td>false</td>
<td>Name of output <code>right</code> view.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;DiffTransform&quot;,
    &quot;name&quot;: &quot;calculate the difference between the yesterday and today datasets&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputLeftView&quot;: &quot;cutomer_20180501&quot;,            
    &quot;inputRightView&quot;: &quot;cutomer_20180502&quot;,            
    &quot;outputIntersectionView&quot;: &quot;customer_unchanged&quot;,            
    &quot;outputLeftView&quot;: &quot;customer_removed&quot;,            
    &quot;outputRightView&quot;: &quot;customer_added&quot;,            
    &quot;persist&quot;: true,
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="httptransform">HTTPTransform</h2>

<h5 id="since-1-0-9-supports-streaming-true">Since: 1.0.9 - Supports Streaming: True</h5>

<p>The <code>HTTPTransform</code> stage transforms the incoming dataset by <code>POST</code>ing the value in the incoming dataset with column name <code>value</code> (must be of type <code>string</code> or <code>bytes</code>) and appending the response body from an external API as <code>body</code>.</p>

<h3 id="parameters-1">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>uri</td>
<td>URI</td>
<td>true</td>
<td>URI of the HTTP server.</td>
</tr>

<tr>
<td>headers</td>
<td>Map[String, String]</td>
<td>false</td>
<td><a href="https://en.wikipedia.org/wiki/List_of_HTTP_header_fields">HTTP Headers</a> to set for the HTTP request. These are not limited to the Internet Engineering Task Force standard headers.</td>
</tr>

<tr>
<td>validStatusCodes</td>
<td>Array[Integer]</td>
<td>false</td>
<td>A list of valid status codes which will result in a successful stage if the list contains the HTTP server response code. If not provided the default values are <code>[200, 201, 202]</code>. Note: all request response codes must be contained in this list for the stage to be successful.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-1">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;HTTPTransform&quot;,
    &quot;name&quot;: &quot;call the machine learning model&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputView&quot;: &quot;cutomers&quot;,            
    &quot;outputView&quot;: &quot;customers_scored&quot;,   
    &quot;outputURI&quot;: &quot;http://internalserver/api/customer_scoring_v101/&quot;,
    &quot;headers&quot;: {
        &quot;Authorization&quot;: &quot;Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==&quot;,
        &quot;custom-header&quot;: &quot;payload&quot;,
    },
    &quot;validStatusCodes&quot;: [200],             
    &quot;persist&quot;: false,
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="jsontransform">JSONTransform</h2>

<h5 id="since-1-0-0-supports-streaming-true">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>JSONTransform</code> stage transforms the incoming dataset to rows of <code>json</code> strings with the column name <code>value</code>. It is intended to be used before stages like <a href="https://aglenergy.github.io/arc/load/#httpload">HTTPLoad</a> or <a href="https://aglenergy.github.io/arc/transform/#httptransform">HTTPTransform</a> to prepare the data for sending externally.</p>

<h3 id="parameters-2">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-2">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;JSONTransform&quot;,
    &quot;name&quot;: &quot;convert customer data to json&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputView&quot;: &quot;cutomers&quot;,            
    &quot;outputView&quot;: &quot;customersJSON&quot;,            
    &quot;persist&quot;: false,
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="metadatafiltertransform">MetadataFilterTransform</h2>

<h5 id="since-1-0-9-supports-streaming-true-1">Since: 1.0.9 - Supports Streaming: True</h5>

<div class="admonition note">
<p class="admonition-title">Experimental</p>
<p><p>The <code>MetadataFilterTransform</code> is currently in experimental state whilst the requirements become clearer.</p>

<p>This means this API is likely to change.</p>
</p>
</div>

<p>The <code>MetadataFilterTransform</code> stage transforms the incoming dataset by filtering columns using the embedded column <a href="../metadata/">metadata</a>.</p>

<p>Underneath Arc will register a table called <code>metadata</code> which contains the metadata of the <code>inputView</code>. This allows complex SQL statements to be executed which returns which columns to retain from the <code>inputView</code> in the <code>outputView</code>. The available columns in the <code>metadata</code> table are:</p>

<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>The field name.</td>
</tr>

<tr>
<td>type</td>
<td>The field type.</td>
</tr>

<tr>
<td>metadata</td>
<td>The field metadata.</td>
</tr>
</tbody>
</table>

<p>This can be used like:</p>

<pre><code class="language-sql">-- only select columns which are not personally identifiable information
SELECT 
    name 
FROM metadata 
WHERE metadata.pii = false
</code></pre>

<p>Will produce an <code>outputView</code> which only contains the columns in <code>inputView</code> where the <code>inputView</code> column metadata contains a key <code>pii</code> which has the value equal to <code>false</code>.</p>

<p>If the <code>sqlParams</code> contains boolean parameter <code>pii_authorized</code> if the job is authorised to use Personally identifiable information or not then it could be used like:</p>

<pre><code class="language-sql">-- only select columns which job is authorised to access based on ${pii_authorized}
SELECT 
    name 
FROM metadata 
WHERE metadata.pii = (
    CASE 
        WHEN ${pii_authorized} = true 
        THEN metadata.pii   -- this will allow both true and false metadata.pii values if pii_authorized = true
        ELSE false          -- else if pii_authorized = false only allow metadata.pii = false values
    END
)
</code></pre>

<p>The <code>inputView</code> and <code>outputView</code> can be set to the same name so that downstream stages have no way of accessing the pre-filtered data accidentially.</p>

<h3 id="parameters-3">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the input file containing the SQL statement.<br><br>This statement must be written to query against a table called <code>metadata</code> and must return at least the <code>name</code> column or an error will be raised.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>sqlParams</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Parameters to inject into the SQL statement before executing. The parameters use the <code>${}</code> format.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-3">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;MetadataFilterTransform&quot;,
    &quot;name&quot;: &quot;filter out Personally identifiable information (pii)&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputURI&quot;: &quot;hdfs://datalake/sql/0.0.1/filterPii.sql&quot;,
    &quot;inputView&quot;: &quot;customerData&quot;,         
    &quot;outputView&quot;: &quot;safeCustomerData&quot;,            
    &quot;persist&quot;: false,
    &quot;authentication&quot;: {
        ...
    },
    &quot;sqlParams&quot;: {
    },       
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="mltransform">MLTransform</h2>

<h5 id="since-1-0-0-supports-streaming-true-1">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>MLTransform</code> stage transforms the incoming dataset with a pretrained Spark ML (Machine Learning) model. This will append one or more predicted columns to the incoming dataset. The incoming model must be a <code>PipelineModel</code> or <code>CrossValidatorModel</code> produced using Spark&rsquo;s Scala, Java, PySpark or SparkR API.</p>

<h3 id="parameters-4">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the input <code>PipelineModel</code> or <code>CrossValidatorModel</code>.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count. MLTransform will also log percentiles of prediction probabilities for classification models if this option is enabled.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-4">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;MLTransform&quot;,
    &quot;name&quot;: &quot;apply machine learning model&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputURI&quot;: &quot;hdfs://input_data/ml/machineLearningPipelineModel.parquet&quot;,
    &quot;inputView&quot;: &quot;inputDF&quot;,         
    &quot;outputView&quot;: &quot;outputDF&quot;,            
    &quot;persist&quot;: false,
    &quot;authentication&quot;: {
        ...
    },
    &quot;params&quot;: {
    }
}
</code></pre>

<h2 id="sqltransform">SQLTransform</h2>

<h5 id="since-1-0-0-supports-streaming-true-2">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>SQLTransform</code> stage transforms the incoming dataset with a <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> statement. This stage relies on previous stages to load and register the dataset views (<code>outputView</code>) and will execute arbitrary SQL statements against those datasets.</p>

<p>All the inbuilt <a href="https://spark.apache.org/docs/latest/api/sql/index.html">Spark SQL functions</a> are available and have been extended with some <a href="https://aglenergy.github.io/arc/partials/#user-defined-functions">additional functions</a>.</p>

<p>Please be aware that in streaming mode not all join operations are available. See: <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#support-matrix-for-joins-in-streaming-queries">Support matrix for joins in streaming queries</a>.</p>

<div class="admonition note">
<p class="admonition-title">CAST vs TypingTransform</p>
<p><p>It is strongly recommended to use the <code>TypingTransform</code> for reproducible, repeatable results.</p>

<p>Whilst SQL is capable of converting data types using the <code>CAST</code> function (e.g. <code>CAST(dateColumn AS DATE)</code>) be very careful. ANSI SQL specifies that any failure to convert then an exception condition is raised: <code>data exception-invalid character value for cast</code> whereas Spark SQL will return a null value and suppress any exceptions: <code>try s.toString.toInt catch { case _: NumberFormatException =&gt; null }</code>. If you used a cast in a financial scenario, for example bill aggregation, the silent <code>NULL</code>ing of values could result in errors being suppressed and bills incorrectly calculated.</p>
</p>
</div>

<h3 id="parameters-5">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the input file containing the SQL statement.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>sqlParams</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Parameters to inject into the SQL statement before executing. The parameters use the <code>${}</code> format.<br><br>For example if the sqlParams contains parameter <code>current_timestamp</code> of value <code>2018-11-24 14:48:56</code> then this statement would execute in a deterministic way: <code>SELECT * FROM customer WHERE expiry &gt; FROM_UNIXTIME(UNIX_TIMESTAMP('${current_timestamp}', 'uuuu-MM-dd HH:mm:ss'))</code> (so would be testable).</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-5">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;SQLTransform&quot;,
    &quot;name&quot;: &quot;Join customer and account&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputURI&quot;: &quot;hdfs://datalake/sql/0.0.1/customerAccountJoin.sql&quot;,
    &quot;outputView&quot;: &quot;customerAccountDF&quot;,            
    &quot;persist&quot;: false,
    &quot;authentication&quot;: {
        ...
    },    
    &quot;sqlParams&quot;: {
        &quot;current_date&quot;: &quot;2018-11-24&quot;,
        &quot;current_timestamp&quot;: &quot;2018-11-24 14:48:56&quot;
    },    
    &quot;params&quot;: {
    }
}
</code></pre>

<p>The <code>current_date</code> and <code>current_timestamp</code> can easily be passed in as environment variables using <code>$(date &quot;+%Y-%m-%d&quot;)</code> and <code>$(date &quot;+%Y-%m-%d %H:%M:%S&quot;)</code> respectively.</p>

<p>The SQL statement is a plain Spark SQL statement, for example:</p>

<pre><code class="language-sql">SELECT 
    customer.customer_id
    ,customer.first_name
    ,customer.last_name
    ,account.account_id
    ,account.account_name
FROM customer
LEFT JOIN account ON account.customer_id = customer.customer_id
</code></pre>

<h2 id="tensorflowservingtransform">TensorFlowServingTransform</h2>

<h5 id="since-1-0-0-supports-streaming-true-3">Since: 1.0.0 - Supports Streaming: True</h5>

<div class="admonition note">
<p class="admonition-title">Experimental</p>
<p><p>The <code>TensorFlowServingTransform</code> is currently in experimental state whilst the requirements become clearer.</p>

<p>This means this API is likely to change.</p>
</p>
</div>

<p>The <code>TensorFlowServingTransform</code> stage transforms the incoming dataset by calling a <a href="https://www.tensorflow.org/serving/">TensorFlow Serving</a> service. Because each call is atomic the TensorFlow Serving instances could be behind a load balancer to increase throughput.</p>

<h3 id="parameters-6">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>uri</td>
<td>String</td>
<td>true</td>
<td>The <code>URI</code> of the TensorFlow Serving REST end point.</td>
</tr>

<tr>
<td>signatureName</td>
<td>String</td>
<td>false</td>
<td>The name of the TensorFlow Serving signature.</td>
</tr>

<tr>
<td>responseType</td>
<td>String</td>
<td>false</td>
<td>The type returned by the TensorFlow Serving API. Expected to be <code>integer</code>, <code>double</code> or <code>object</code>.</td>
</tr>

<tr>
<td>batchSize</td>
<td>Int</td>
<td>false</td>
<td>The number of records to sent to TensorFlow Serving in each call. A higher number will decrease the number of calls to TensorFlow Serving which may be more efficient</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-6">Examples</h3>

<pre><code class="language-json">{
    &quot;environments&quot;: [&quot;prd&quot;,&quot;tst&quot;],
    &quot;type&quot;: &quot;TensorFlowServingTransform&quot;,
    &quot;name&quot;: &quot;call the customer segmentation model&quot;,
    &quot;inputView&quot;: &quot;customer&quot;,
    &quot;outputView&quot;: &quot;customer_segmented&quot;,            
    &quot;uri&quot;: &quot;http://tfserving:9001/v1/models/customer_segmentation/versions/1:predict&quot;,
    &quot;signatureName&quot;: &quot;serving_default&quot;,
    &quot;batchSize&quot;: 100,
    &quot;persist&quot;: true,
    &quot;params&quot;: {}
}   
</code></pre>

<h2 id="typingtransform">TypingTransform</h2>

<h5 id="since-1-0-0-supports-streaming-true-4">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>TypingTransform</code> stage transforms the incoming dataset with based on metadata defined in the <a href="../metadata/">metadata</a> format.</p>

<p>The logical process that is applied to perform the typing on a field-by-field basis is shown below.</p>

<h3 id="parameters-7">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the input file containing the SQL statement.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service prior to extract execution. See <a href="../partials/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>failMode</td>
<td>String</td>
<td>false</td>
<td>Either <code>permissive</code> or <code>failfast</code>:<br><br><code>permissive</code> will process all rows in the dataset and collect any errors for each row in the <code>_errors</code> column. Rules can then be applied in a <a href="validate/#sqlvalidate">SQLValidate</a> stage if required.<br><br><code>failfast</code> will fail the Arc job on the first row containing at least one error.<br><br>Default: <code>permissive</code>.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>
</tbody>
</table>

<h3 id="examples-7">Examples</h3>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;TypingTransform&quot;,
    &quot;name&quot;: &quot;apply data types to customer records&quot;,
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    &quot;inputURI&quot;: &quot;hdfs://datalake/meta/0.0.1/customer_meta.json&quot;,
    &quot;inputView&quot;: &quot;customerUntypedDF&quot;,            
    &quot;outputView&quot;: &quot;customerTypeDF&quot;,            
    &quot;persist&quot;: false,
    &quot;authentication&quot;: {
        ...
    },       
    &quot;params&quot;: {
    }
}
</code></pre>

<p>A demonstration of how the <code>TypingTransform</code> behaves. Assuming you have read an input like a <a href="../extract/#DelimitedExtract">DelimitedExtract</a> which will read a dataset where all the columns are read as strings:</p>

<pre><code class="language-bash">+-------------------------+---------------------+
|startTime                |endTime              |
+-------------------------+---------------------+
|2018-09-26 07:17:43      |2018-09-27 07:17:43  |
|2018-09-25 08:25:51      |2018-09-26 08:25:51  |
|2018-02-30 01:16:40      |2018-03-01 01:16:40  |
|30 February 2018 01:16:40|2018-03-2018 01:16:40|
+-------------------------+---------------------+
</code></pre>

<p>In this case the goal is  to safely convert the values from strings like <code>&quot;2018-09-26 07:17:43&quot;</code> to a proper <code>timestamp</code> object so that we can ensure the timestamp is valid (e.g. not on a date that does not exist e.g. the 30 day of February) and can easily perform date operations such as subtracting 1 week. To do so a <a href="../metadata/">metadata</a> file could be constructed to look like:</p>

<pre><code class="language-json">[
  {
    &quot;id&quot;: &quot;8e42c8f0-22a8-40db-9798-6dd533c1de36&quot;,
    &quot;name&quot;: &quot;startTime&quot;,
    &quot;description&quot;: &quot;The startTime field.&quot;,
    &quot;type&quot;: &quot;timestamp&quot;,
    &quot;trim&quot;: true,
    &quot;nullable&quot;: true,
    &quot;nullableValues&quot;: [
        &quot;&quot;,
        &quot;null&quot;
    ],
    &quot;formatters&quot;: [
        &quot;uuuu-MM-dd HH:mm:ss&quot;
    ],
    &quot;timezoneId&quot;: &quot;UTC&quot;
  },
  {
    &quot;id&quot;: &quot;2e7553cf-2748-49cd-a291-8918823e706a&quot;,
    &quot;name&quot;: &quot;endTime&quot;,
    &quot;description&quot;: &quot;The endTime field.&quot;,
    &quot;type&quot;: &quot;timestamp&quot;,
    &quot;trim&quot;: true,
    &quot;nullable&quot;: true,
    &quot;nullableValues&quot;: [
        &quot;&quot;,
        &quot;null&quot;
    ],
    &quot;formatters&quot;: [
        &quot;uuuu-MM-dd HH:mm:ss&quot;
    ],
    &quot;timezoneId&quot;: &quot;UTC&quot;
  }   
]
</code></pre>

<p>Here is the output of the <code>TypingTransformation</code> when applied to the input dataset.</p>

<pre><code class="language-bash">+-------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|startTime          |endTime            |_errors                                                                                                                                                                                                                                                             |
+-------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|2018-09-26 17:17:43|2018-09-27 17:17:43|[]                                                                                                                                                                                                                                                                  |
|2018-09-25 18:25:51|2018-09-26 18:25:51|[]                                                                                                                                                                                                                                                                  |
|null               |2018-03-01 12:16:40|[[startTime, Unable to convert '2018-02-30 01:16:40' to timestamp using formatters ['uuuu-MM-dd HH:mm:ss'] and timezone 'UTC']]                                                                                                                                     |
|null               |null               |[[startTime, Unable to convert '28 February 2018 01:16:40' to timestamp using formatters ['uuuu-MM-dd HH:mm:ss'] and timezone 'UTC'], [endTime, Unable to convert '2018-03-2018 01:16:40' to timestamp using formatters ['uuuu-MM-dd HH:mm:ss'] and timezone 'UTC']]|
+-------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
</code></pre>

<ul>
<li>Because the conversion happened successfully for both values on the first two rows there are no errors for those rows.</li>
<li>On the third row the value <code>'2018-02-30 01:16:40'</code> cannot be converted as the 30th day of February is not a valid date and the value is set to <code>null</code>. If the <code>nullable</code> in the <a href="../metadata/">metadata</a> for field <code>startTime</code> was set to <code>false</code> the job would fail as it would be unable to continue.</li>
<li>On the forth row both rows are invalid as the <code>formatter</code> and <code>date</code> values are both wrong.</li>
</ul>

<p>The <a href="../validate/#sqlvalidate">SQLValidate</a> stage is a good way to use this data to enforce data quality constraints.</p>

<h3 id="logical-flow">Logical Flow</h3>

<p>The sequence that these fields are converted from <code>string</code> fields to <code>typed</code> fields is per this flow chart. Each value and its typing metadata is passed into this logical process. For each row the <code>values</code> are returned as standard table columns and the returned <code>error</code> values are groupd into a field called <code>_errors</code> on a row-by-row basis. Patterns for consuming the <code>_errors</code> array is are demonstrated in the <a href="validate/#sqlvalidate">SQLValidate</a> stage.</p>

<p><img src="https://aglenergy.github.io/arc/img/typing_flow.png" alt="Logical Flow for Data Typing" title="Logical Flow for Data Typing" /></p>


			<aside class="copyright" role="note">
				
				&copy; 2018 Released under the MIT license
				
			</aside>

			<footer class="footer">
				

<nav class="pagination" aria-label="Footer">
  <div class="previous">
  
      <a href="https://aglenergy.github.io/arc/extract/" title="Extract">
        <span class="direction">
          Previous
        </span>
        <div class="page">
          <div class="button button-previous" role="button" aria-label="Previous">
            <i class="icon icon-back"></i>
          </div>
          <div class="stretch">
            <div class="title">
              Extract
            </div>
          </div>
        </div>
      </a>
  
  </div>

  <div class="next">
  
      <a href="https://aglenergy.github.io/arc/load/" title="Load">
        <span class="direction">
          Next
        </span>
        <div class="page">
          <div class="stretch">
            <div class="title">
              Load
            </div>
          </div>
          <div class="button button-next" role="button" aria-label="Next">
            <i class="icon icon-forward"></i>
          </div>
        </div>
      </a>
  
  </div>
</nav>





			</footer>
		</div>
	</article>

	<div class="results" role="status" aria-live="polite">
		<div class="scrollable">
			<div class="wrapper">
				<div class="meta"></div>
				<div class="list"></div>
			</div>
		</div>
	</div>
</main>

    <script>
    
      var base_url = 'https:\/\/aglenergy.github.io\/arc\/';
      var repo_id  = 'aglenergy\/arc';
    
    </script>

    <script src="https://aglenergy.github.io/arc/javascripts/application.js"></script>
    

    <script>
      /* Add headers to scrollspy */
      var headers   = document.getElementsByTagName("h2");
      var scrollspy = document.getElementById('scrollspy');

      if(scrollspy) {
        if(headers.length > 0) {
          for(var i = 0; i < headers.length; i++) {
            var li = document.createElement("li");
            li.setAttribute("class", "anchor");

            var a  = document.createElement("a");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", headers[i].innerHTML);
            a.innerHTML = headers[i].innerHTML;

            li.appendChild(a)
            scrollspy.appendChild(li);
          }
        } else {
          scrollspy.parentElement.removeChild(scrollspy)
        }


        /* Add permanent link next to the headers */
        var headers = document.querySelectorAll("h1, h2, h3, h4, h5, h6");

        for(var i = 0; i < headers.length; i++) {
            var a = document.createElement("a");
            a.setAttribute("class", "headerlink");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", "Permanent link")
            a.innerHTML = "#";
            headers[i].appendChild(a);
        }
      }
    </script>

    

    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/languages/scala.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </body>
</html>

